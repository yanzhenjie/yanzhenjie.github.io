[{"title":"人工智能|GPU加速PyTorch训练","url":"/post/20240313/a6452f4cd3d9/","content":"1、GPU 的并行计算我在人工智能|各名称与概念之介绍一文中提到过，深度学习是目前最主流的人工智能算法，从过程来看，它包括训练（Training）和推理（Inference）两个环节。\n在训练环节，通过投喂大量的数据，训练出一个复杂的神经网络模型。在推理环节，利用训练好的模型，使用大量数据推理出各种结论。\n训练环节由于涉及海量的训练数据和复杂的深度神经网络结构，所以计算规模非常庞大，对芯片的算力性能要求比较高。而推理环节，对简单指定的重复计算和实时性要求很高。模型所采用的具体算法，包括矩阵相乘、全连接层、卷积层、池化层、激活层、批归一化层和梯度运算等，分解为大量并行任务，可以有效缩短任务完成的时间。\n总之，我想说明的是：神经网络模型的训练需要庞大的算力。那么它和 GPU 有什么关系呢？CPU 又在做什么？\n理解 CPU 的角色。如果我们把处理器看成是一个餐厅的话，CPU 就像一个拥有几十名高级厨师的全能型餐厅，它什么菜系的菜都能做。但是因为菜系众多、工序复杂和厨师较少，所以需要花费大量的时间协调和配菜，上菜的速度相对比较慢。\n理解 GPU 的角色。如果我们把处理器看成是一个餐厅的话，GPU 就像一个拥有千万名初级厨师的单一型餐厅，它只适合做某种指定菜系的菜。但是因为菜系单一、工序简单和厨师较多，所以仅需要少量时间通知任意一个厨师做菜，上菜速度反而快。\n（图一）\n\n如图一所示，左侧为 CPU 角色，右侧为 GPU 角色。GPU 凭借自身强悍的并行计算能力以及内存带宽，可以很好地应对训练和推理任务，已经成为业界在深度学习领域的首选解决方案。如果对模型进行合理优化，一块 GPU 可以提供相当于几十块 CPU 的算力。\n\n本小节内容引用自（有大量修改，侵删）：https://www.toutiao.com/article/7319434384707158562\n\n2、CUDA 和 MPStorch.cuda和torch.mps是 PyTorch 中的 2 个不同的模块，用于加速 PyTorch 训练。开发者在安装 PyTorch 时就需要注意安装正确的版本。我们进入官网后可以看到本地部署引导，并且会默认选中是和我们电脑的版本并给出安装命令或者安装包地址。\n我们可以用鼠标点击选择自己想要安装的版本（系统、包管理、语言、计算平台）：\n（图二）\n\n比如本人使用的电脑为 MBP M2，可以选择稳定版 PyTorch、MacOS、Pip、Python、计算平台为 Default，出给出安装命令：\npip3 install torch torchvision torchaudio\n\n需要注意的是，MBP M 芯片系列的电脑，计算平台中 CUDA 会被画横线，不能选择，如果强行选择则不能安装。\n\n官网地址：https://pytorch.org/get-started/locally/\n存档版本：https://pytorch.org/get-started/previous-versions\n\n2.1、非 M 芯片验证 CUDA 可用性如果是非 M 系列芯片的电脑（OS X、Linux、Window），安装命令大概率是包含 cadu 的：\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorchconda install pytorch torchvision torchaudio cudatoolkit -c pytorch\n\n此时可以直接验证 GPU 状态是否正常可用：\nimport torchtorch.cuda.is_available()\n\n如果正常可用，将输出：\nTrue\n\n如果不正常，将输出：\nFalse\n\ntorch.cuda的详细 API 文档：https://pytorch.org/docs/master/cuda.html\n2.2、M 芯片验证 MPS 可用性如果是 M 系列芯片，那么是无法安装 CUDA 的：\n（图三）\n\n很早之前，PyTorch 是不支持 M 芯片的。我查了下相关信息，找到 soumith 在 PyTorch 项目下讨论过对 GPU 加速的支持：https://github.com/pytorch/pytorch/issues/47702，2020 年 10 月的回答，有兴趣的读者可以看看。\n在 2022 年 5 月 18 日，PyTorch 发布博客说 PyTorch v1.12 版本支持了 Mac OS 的 GPU 加速：https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/。\n苹果的 MPS（Metal Performance Shaders）扩展了 PyTorch 框架，可以作为 PyTorch 的后端加速 GPU 训练，它提供了在 Mac 上设置和运行操作的脚本和功能，MPS 通过针对每个 Metal GPU 系列的独特特性进行了微调的内核来优化计算性能。\n如果你安装的 PyTorch 的版本是大于等于 v1.12 的，那么默认就支持加速 PyTorch 训练的。\n此时可以直接验证 GPU 状态是否正常可用：\nimport torchtorch.backends.mps.is_available()\n\n如果正常可用，此时将输出：\nTrue\n\n如果不正常，此时将输出：\nFalse\n\ntorch.mps的详细 API 文档：\n\nhttps://pytorch.org/docs/master/mps.html\nhttps://pytorch.org/docs/master/notes/mps.html\n\n3、CPU 和 GPU 的切换首先需要确定在设备上有 1 个或者多个 GPU 可用，在 2 中我们已经确认过了。现在我们需要将我们的数据转移到 GPU 可以看到的地方，是什么意思呢？我们知道 CPU 做计算靠的是 RAM，GPU 做计算靠的是 VRAM（Video Random Access Memory），VRAM 是 GPU 的专用内存。因此我们可以说把数据移动到 GPU 连接的内存，或者简称把数据移动到 GPU。\n默认情况下，我们创建张量是在 CPU 设备上的（打印张量时，通过张量的属性可以看到）：\nimport torchx = torch.Tensor([1, 2, 3])print(x, x.device)\n\n此时可以看到输出：\ntensor([1., 2., 3.]) cpu\n\n以torch.rand()函数为例，如果我们事先知道此时 GPU 的可用状态，那么我们直接创建即可：\nif torch.cuda.is_available():    my_device = torch.device(&#x27;cuda&#x27;)elif torch.backends.mps.is_available():    my_device = torch.device(&#x27;mps&#x27;)else:    my_device = torch.device(&#x27;cpu&#x27;)print(&#x27;Device: &#123;&#125;&#x27;.format(my_device))x = torch.rand(2, 2, device=my_device)print(x)\n\n如果 GPU 支持cuda，此时可以看到输出：\nDevice: cudatensor([[0.0024, 0.6778],        [0.2441, 0.6812]], device=&#x27;cuda:0&#x27;)\n\n如果 GPU 支持mps，此时可以看到输出：\nDevice: mpstensor([[0.5986, 0.4086],        [0.0624, 0.7131]], device=&#x27;mps:0&#x27;)\n\n当然，每次写一堆if else会感觉比较费劲，可以用三元表达式简写它们：\ndevice_name = (    &quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot;    if torch.backends.mps.is_available() else &quot;cpu&quot;)print(device_name)\n\nOK，现在我们可以使用.to函数让张量在 CPU 和 GPU 之间进行切换了：\n# 先拿到处理器名称device_name = (    &quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot;    if torch.backends.mps.is_available() else &quot;cpu&quot;)# 声明处理器cpu_device = torch.device(&quot;cpu&quot;)gpu_device = torch.device(device_name)# 声明张量x = torch.Tensor([1, 2, 3])# 切换到CPUx = x.to(cpu_device)print(x, x.device)# 切换到GPUx = x.to(gpu_device)print(x, x.device)\n\n此时可以看到输出：\ntensor([1., 2., 3.]) cputensor([1., 2., 3.], device=&#x27;mps:0&#x27;) mps:0\n\nPyTorch 官网一个很老的文档，但不及本文新：https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html#moving-to-gpu\n有兴趣一起交流学习的，可以加我的QQ群：874450808\n本文完！\n","categories":["人工智能"],"tags":["PyTorch"]},{"title":"Android Perfetto Trace性能分析","url":"/post/20240406/b3b882a8973b/","content":"本人用 Perfetto 快 2 年了，公司一些同事和网友都来咨询过使用姿势，本人只能把自己一个略显粗糙点的笔记发给同事，有点不够细节和不够深度，于是趁着清明假期，我把经验总结成一篇博客，结合 Perfetto 官网资料取长补短尽量做到深入浅出。\n\n\nPerfetto 支持多个平台，包括 Linux、Android 和 Chrome，并提供了用于记录系统级和应用级活动的服务和库、低开销的针对 Native 和 Java 的内存分析工具、可供 SQL 分析跟踪文件的 C++库和 Python 库（Python 基于 C++库），以及基于 Web 可视化方便分析 Trace 文件的 Perfetto UI。\n\n快速开始，Android 抓 Trace：\nhttps://perfetto.dev/docs/quickstart/android-tracing\n\n\n可视化工具，Perfetto UI：\n地址：https://ui.perfetto.dev/\n教程：https://perfetto.dev/docs/visualization/perfetto-ui\n\n\n开源：\n地址：https://github.com/google/perfetto\n贡献：https://perfetto.dev/docs/contributing/getting-started\n\n\n\n聪明的同学从上面的信息中可以了解到很多信息了，下面咱们就展开细节聊一聊。\n1、生成 Perfetto 抓 Trace 的配置做 App 性能分析，一般关注这几类信息：\n\nMemory。\nCPU。\nGPU。\nPower。\n系统层消耗。\n\n我们在抓 Trace 的时候可以指定应用、内存、CPU、GPU、电源和系统事件等，使用 Perfetto 时可以使用 PerfettoUI 来生成配置：https://ui.perfetto.dev/#!&#x2F;record。\n第一步，选择目标 Android 设备系统版本。打开 Perfetto UI，选择要抓 Trace 的目标设备，有 2 种方式，如图一所示：\n\n方式一，选择目标系统版本。\n方式二，选择 ADB 连接的设备。\n\n（图一）\n\n第二步，调整 Trace 文件记录设置。主要是为了控制 Trace 文件大小或者 Trace 记录时长，如果文件太大则加载缓慢操作也不太顺畅，如图二所示：\n（图二）\n\n第三步，配置要分析的探针信息。\n收集 CPU 相关信息，需要点击在 Probes 下的 CPU，然后根据需要打开对应开关并做配置，如图三所示：\n（图三）\n\n收集 GPU 相关信息，需要点击在 Probes 下的 GPU，然后根据需要打开对应开关，如图四所示：\n（图四）\n\n收集电源信息和内存信息同上，不再赘述。\n这里需要强调收集 atrace 的配置，选中 Probes 下的 Android apps &amp; avcs，如图五所示：\n（图五）\n\n按照以下操作步骤和注意项：\n\n打开 atrace 开关。\n默认是抓取所有 app trace，可以关闭Record events from Android apps and services，在下面添加目标 App 包名。\n在 Categories 下选择你关注的 trace 类型。\n\n第四步，选择【Recording command】保存配置，一共有 3 种方式。\n方式一，保存在本地，打开【Recording command】的页面后，点击按钮复制配置内容，如图六所示：\n（图六）\n\n方式二，在 Perfetto UI 持久化保存，如图七所示：\n（图七）\n\n方式三，生成远端链接分享给别人下载，如图八所示：\n（图八）\n\n我们可以多尝试下配置会更加熟悉和了解细节，保存后可以根据文档手动增删改一些配置（见下文），比如我们主要关注 CPU 信息时，做好对应设置生成的配置如下：\nbuffers: &#123;    size_kb: 63488    fill_policy: DISCARD&#125;buffers: &#123;    size_kb: 2048    fill_policy: DISCARD&#125;data_sources: &#123;    config &#123;        name: &quot;android.packages_list&quot;        target_buffer: 1    &#125;&#125;data_sources: &#123;    config &#123;        name: &quot;android.gpu.memory&quot;    &#125;&#125;data_sources: &#123;    config &#123;        name: &quot;linux.process_stats&quot;        target_buffer: 1        process_stats_config &#123;            scan_all_processes_on_start: true        &#125;    &#125;&#125;data_sources: &#123;    config &#123;        name: &quot;linux.sys_stats&quot;        sys_stats_config &#123;            stat_period_ms: 1000            stat_counters: STAT_CPU_TIMES            stat_counters: STAT_FORK_COUNT            #cpufreq_period_ms: 1000        &#125;    &#125;&#125;data_sources: &#123;    config &#123;        name: &quot;linux.ftrace&quot;        ftrace_config &#123;            ftrace_events: &quot;sched/sched_switch&quot;            ftrace_events: &quot;power/suspend_resume&quot;            ftrace_events: &quot;sched/sched_wakeup&quot;            ftrace_events: &quot;sched/sched_wakeup_new&quot;            ftrace_events: &quot;sched/sched_waking&quot;            ftrace_events: &quot;power/cpu_frequency&quot;            ftrace_events: &quot;power/cpu_idle&quot;            ftrace_events: &quot;power/gpu_frequency&quot;            ftrace_events: &quot;gpu_mem/gpu_mem_total&quot;            ftrace_events: &quot;power/gpu_work_period&quot;            ftrace_events: &quot;raw_syscalls/sys_enter&quot;            ftrace_events: &quot;raw_syscalls/sys_exit&quot;            ftrace_events: &quot;sched/sched_process_exit&quot;            ftrace_events: &quot;sched/sched_process_free&quot;            ftrace_events: &quot;task/task_newtask&quot;            ftrace_events: &quot;task/task_rename&quot;            ftrace_events: &quot;ftrace/print&quot;            atrace_categories: &quot;am&quot;            atrace_categories: &quot;aidl&quot;            atrace_categories: &quot;dalvik&quot;            atrace_categories: &quot;binder_lock&quot;            atrace_categories: &quot;camera&quot;            atrace_categories: &quot;database&quot;            atrace_categories: &quot;gfx&quot;            atrace_categories: &quot;network&quot;            atrace_categories: &quot;sm&quot;            atrace_categories: &quot;ss&quot;            atrace_categories: &quot;view&quot;            atrace_categories: &quot;webview&quot;            atrace_apps: &quot;com.taobao.android&quot;        &#125;    &#125;&#125;duration_ms: 40000\n\n\n实际运行时，因为手机系统版本不一致原因，可能遇到以下错误：\n-:36:13 error: No field named &quot;cpufreq_period_ms&quot; in proto SysStatsConfig            cpufreq_period_ms: 1000\n\n如上面给出的配置示例，注释对应配置行后再尝试。\n\n在实际操作中，会根据不同的场景和需求对抓 Trace 的配置文件做修改，如何修改可以参考官网文档（https://perfetto.dev/docs/）数据源配置，文档位置如图九所示：\n（图九）\n\n生成后好放在电脑的本地（比如桌面），可以把它命名为 perfetto.pbtx。\n2、按照配置抓 Perfetto Trace让手机抓 Perfetto Trace 有 3 种方式：\n\n使用手机的 Traceur app 抓取，很多国产手机没有，本文不做介绍。\n使用 adb 命令抓取。\n使用 Perfetto UI 抓取。\n使用 Python 脚本抓取（推荐）。\n\n值得注意的是 Perfetto 从 Android 9（P）开始集成，从 Android 11（R）开始默认开启。在 Android 9（P）和 Android 10（Q）上需要先确保开启 Trace 服务：\nadb shell setprop persist.traced.enable 1\n\n\n如果你要在 Android 9（P）之前的设备上使用 Perfetto，请参考官网文档：https://perfetto.dev/docs/quickstart/android-tracing#recording-a-trace-through-the-cmdline\n\n如果执行了抓取后，没有抓到对应 App 的 Trace，启用systrace VERBOSE再试试：\nadb shell setprop log.tag.enableSystrace VERBOSE\n\n2.1、使用 adb 命令抓取\n通过 adb 把配置推送到手机：\nadb push ~/Desktop/perfetto.pbtx /data/local/tmp/perfetto.pbtx\n\n\n使用 adb 让手机以指定配置抓 Perfetto Trace：\nadb shell &#39;cat /data/local/tmp/perfetto.pbtx | perfetto --txt -c - -o /data/misc/perfetto-traces/trace&#39;\n\n\n结束抓取：\nadb shell &#39;perfetto --attach=perf_debug --stop&#39;\n\n\n\n抓取完成后，使用 adb 命令导出 Trace 文件到电脑，进行分析即可。该方式操作步骤多略显繁琐，根据实际情况作为备用方式。\n2.2、使用 Perfetto UI 抓取使用 Perfetto UI 抓取 Trace 时，选择目标 Android 设备系统版本时，需要选择 ADB 连接的设备，在 Perfetto UI 上配置完成后，不用保存到本地，直接点击【Start Recording】开始录制，如图十所示：\n（图十）\n\n在 Perfetto UI 抓取 Trace 时不能手动结束，需要等在【Recording Setting】中设置的【Max Duration】倒计时结束，当我们点击了录制后会看到如下提示：\nRecording in progress for 40000 ms...\n\n录制结束后，Perfetto UI 会直接在 Web 界面打开 Trace 文件，我们可以直接进行分析。该方式比较方便，配置可以在 Perfetto UI 持久化保存，美中不足的是 Perfetto UI 抓到的 Trace 文件不能下载发给别人。\n2.3、使用 Python 脚本抓取使用 Python 脚本抓取到 Trace 后，会把 Trace 文件保存到本地，也会自动在浏览器通过 Perfetto UI 直接打开 Trace 文件，我们直接进行分析。\n使用 Python 脚本抓取时需要满足以下几个条件：\n\nAndroid 设备通过 adb 连接到电脑。\n把 Python 脚本保存在本地，在本地运行 Python 脚本。\n把抓 Trace 的配置保存在本地，运行 Python 脚本时需要指定配置文件。\n\nPython 脚本在 GitHub 上的开源地址，可以通过浏览器看源码：https://github.com/google/perfetto/blob/main/tools/record_android_traceraw.githubusercontent.com 下载链接：https://raw.githubusercontent.com/google/perfetto/master/tools/record_android_trace\n现在我们把 Python 脚本和抓 Trace 的配置放在桌面，命名和目录结构如下：\n~/Desktop$├── perfetto.py├── perfetto.pbtx\n\n此时我们手机与电脑通过 adb 连接，然后运行以下命令抓取 Trace：\npython3 perfetto.py -c perfetto.pbtx -o trace_file.perfetto-trace\n\n\n上述命令中，-c是指定配置文件位置，-o是指定 trace 文件保存位置。\n\n运行命令后，我们开始操作 App，然后觉得抓取到目标 Trace 了，按下ctrl + c结束即可，此时 Trace 文件会被放在-o指定的位置，且 Perfetto UI 会被自动打开，我们直接进行分析即可。\n\n而且我们不用担心抓的 Trace 太大网站内存绷不住，因为 Python 脚本会自动下载当前系统的 trace_processor 来解析，细节可以从官网了解。https://perfetto.dev/docs/visualization/large-traces\n\n3、打 Trace先大概了解下 Trace 的基本概念，一个 Trace 段在 Perfetto UI 中用 Slice 表示，它并不代表一个方法，而是一个 Trace 的开始到结束，需要在代码中标记。\nTrace.beginSection(&quot;Choreographer#doFrame&quot;);...Trace.endSection();\n\n如上述代码所示，这是一个同步 Trace，一个同步 Trace 段必须是一个Trace#beginSection(String)对应一个Trace#endSection()，否则抓取到的 Trace 将会异常。\n3.1、同步 TraceTrace 也可以嵌套多层，比如我们知道某个方法很耗时，但是该方法由很多代码段组成，那我们就可以给该方法中每一段代码加上 Trace，来分析具体是哪一段代码引起的耗时，例如：\npublic void test() &#123;    Trace.beginSection(&quot;First&quot;);    ...;        Trace.beginSection(&quot;Second&quot;);        ...;            Trace.beginSection(&quot;Third&quot;);            ...;            Trace.endSection();        Trace.endSection();    Trace.endSection();    Trace.beginSection(&quot;AAA&quot;);    ...;        Trace.beginSection(&quot;BBB&quot;);        ...;        Trace.endSection();    Trace.endSection();&#125;\n\n\n注意：同步 Trace 的Trace.beginSection(String)和Trace.endSection()需要在同一个线程调用。\n\n如上代码，为了方便看清结构，我添加了缩进，我们将得到如图十一所示的 Trace：\n（图十一）\n\n3.2、异步 Trace我认为，异步 Trace 具有以下特点：\n\n异步 Trace 可以解决同步 Trace 不能跨线程的问题。\n异步 Trace 不用考虑begin和end是否在同一个线程。\n异步 Trace 可以观察流程耗时，比同步 Trace 更宏观。\n\n3.2.1、同步 Trace 的局限性我们先看一个示例，我们想统计request()方法和工作线程的整体耗时：\npublic void request() &#123;    ThreadPoolExecutor executors = ...;    Trace.beginSection(&quot;request&quot;);    ...;    executors.execute(new Runnable() &#123;        @Override        public void run() &#123;            ...;            notifyReuslt();        &#125;    &#125;);    Trace.endSection();&#125;\n\n如上代码所示，其结果只是统计了ThreadPoolExecutor#execute的耗时，工作线程的耗时没有统计到。\n\n上文提到过，同步 Trace 不能跨线程使用，因此Trace.endSection()不能放在run()结尾。\n\n依然是如上需求，我们换一种姿势：\npublic void request() &#123;    ThreadPoolExecutor executors = ...;    Trace.beginSection(&quot;request&quot;);    ...;    executors.execute(new Runnable() &#123;        @Override        public void run() &#123;            Trace.beginSection(&quot;request#run&quot;);            ...;            notifyReuslt();            Trace.endSection();        &#125;    &#125;);    Trace.endSection();&#125;\n\n这段代码，抓取 Trace 后会，2 个 Trace 会分别分布在主线程和工作线程，虽然可以人工计算得出整体耗时，但是非常不方便、不直观。如果在工作流程中切换了好几个线程，就更加麻烦了。\n\n大胆想象，大型应用中对统计切换多个线程的整体耗时情况非常常见，下文会介绍\n\n3.2.2、异步 Trace 使用异步 Trace 的基本使用：\n// TraceUtils.javapublic static void beginAsync(String name, int cookie) &#123;    if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.Q) &#123;        Trace.beginAsyncSection(name, cookie);    &#125;&#125;public static void endAsync(String name, int cookie) &#123;    if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.Q) &#123;        Trace.endAsyncSection(name, cookie);    &#125;&#125;\n\n例如，还是以上述代码为例，我们统计request()整体耗时：\npublic void request() &#123;    ThreadPoolExecutor executors = ...;    TraceUtils.beginAsync(&quot;request&quot;, 1);    ...;    executors.execute(new Runnable() &#123;        @Override        public void run() &#123;            ...;            TraceUtils.beginAsync(&quot;handleData&quot;, 1);            ...;            TraceUtils.endAsync(&quot;handleData&quot;, 1);            notifyReuslt();            TraceUtils.endAsync(&quot;request&quot;, 1);        &#125;    &#125;);&#125;public void notifyResult() &#123;    ThreadUtils.runOnMain(new Runnable() &#123;        @Override        public void run() &#123;            TraceUtils.beginAsync(&quot;notifyResult&quot;, 1);            ...;            TraceUtils.endAsync(&quot;notifyResult&quot;, 1);        &#125;    &#125;);&#125;\n\n我们将得到如图十二所示 Trace：\n（图十二）\n\n再例如，当A1-request整个流程跨越了好几个线程时，使用同步 Trace 是无法将它们聚合到一起的，如果使用异步 Trace，那么我们可以得到如图十三所示 Trace：\n（图十三）\n\n在复杂的场景的性能分析和优化是非常有用的，比如 App 冷启动，有很多初始化任务、首页请求任务、首页渲染、图像加载等，非常依赖异步 Trace 做流程分析。\n3.2.3、异步 Trace 封装通过上文我们看到，打异步 Trace 时除了要传一个name还要传一个cookie，它的原理就和token一样，因为异步 Trace 跨越多个线程，也可能同一个name发生多次，所以需要cookie来做流程唯一标记，否则第二次发生就会覆盖第一次发生。\n同时，当我们一次流程明确时，上述使用方式就极为不便（需要记录 cookie），而且当流程结束时，流程中一些尚未标记的异步 Trace（可能是一些后续完成的异步任务）发生时，会打乱该流程中的 Trace 记录。\n封装一个自动处理cookie的AsyncTrace：\n// TraceUtils.java// 异步Trace，cookie生成器private static final AtomicInteger COOKIE_MAKER = new AtomicInteger();private static class Async &#123;    private final int cookie;    private boolean finished;    public Async() &#123;        cookie = COOKIE_MAKER.incrementAndGet();    &#125;    public void begin(String methodName) &#123;        if (!finished) &#123;            TraceUtils.beginAsync(methodName, cookie);        &#125;    &#125;    public void end(String methodName) &#123;        if (!finished) &#123;            TraceUtils.endAsync(methodName, cookie);        &#125;    &#125;    public void finish() &#123;        finished = true;    &#125;    public boolean isFinished() &#123;        return finished;    &#125;&#125;\n\n当我们new Async()生成一个实例时，这个 cookie 将会被记录，使用这个实例记录的所以 Trace 的begin和end对的cookie将自动统一。\n为了方便获取一个流程（比如页面秒开）的实例，还需要封装获取实例的类：\n// TraceUtils.java// 异步Trace列表private static final Map&lt;String, Async&gt; ASYNC_LIST = new ConcurrentHashMap&lt;&gt;();/** * @param biz 流程名称。 * @return Async */public static Async async(String biz) &#123;    Async async = ASYNC_LIST.get(biz);    if (async != null &amp;&amp; async.isFinished()) &#123;        ASYNC_LIST.remove(biz);        async = null;    &#125;    if (async == null) &#123;        async = new AsyncTrace();        ASYNC_LIST.put(biz, async);    &#125;    return async;&#125;public static Async launch() &#123;    return TraceUtils.async(&quot;launch&quot;);&#125;\n\n使用上述简单的封装，我们的使用姿势将是这样：\nHarryTrace.async(biz).begin(&quot;A1-request&quot;);...;    HarryTrace.async(biz).begin(&quot;A2-connect&quot;);    ...;    HarryTrace.async(biz).end(&quot;A2-connect&quot;);    HarryTrace.async(biz).begin(&quot;A3-sendData&quot;);    ...;    HarryTrace.async(biz).end(&quot;A3-sendData&quot;);HarryTrace.async(biz).end(&quot;A1-request&quot;);\n\n3.2.4、复杂异步 Trace 示例如 3.2.2 中末尾所示，我们可以在一个复杂的流程中使用异步 Trace 方便我们分析整个流程的耗时情况，对于每一个阶段的方法再使用同步 Trace 分析具体代码耗时。\n给一个实际应用中复杂的例子，这是从 App 启动、预定位、预加载、首页渲染完成到用户可交互的耗时情况，由于截图弄不了那么大，没有加应用启动阶段Application#onCreate的耗时情况，如图十四所示：\n（图十四）\n\n3.3、常见问题文章发出后，有人问我为什么 Release 包抓不到 Trace？我总结了以下几点（不同版本系统不一样，建议以下选项可以全部尝试配置）：\n\n执行 adb 命令开启 Trace，上文中有提到。\n\nadb shell setprop persist.traced.enable 1adb shell setprop log.tag.enableSystrace VERBOSE\n\n\n检查 manifest.xml 中是否配置了 profileable。\n官网文档：https://developer.android.com/guide/topics/manifest/profileable-element\n\n\n是否在打 Release 包时去掉了 Trace，一般是打包插件做的，不同公司方式不同。（最有可能的）\n系统可能禁用了 App Trace，可以反射开启，也可以包装 Trace 类在发布版中不打 Trace。\n\nprivate static boolean initialized = false;private static void startTrace() &#123;    boolean allowTrace = BuildConfig.TRACE;    if (!allowTrace || initialized) &#123;        return;    &#125;    initialized = true;    Class&lt;?&gt; trace = Trace.class;    try &#123;        //noinspection JavaReflectionMemberAccess        @SuppressLint(&quot;DiscouragedPrivateApi&quot;)        Method setAppTracingAllowed = trace.getDeclaredMethod(&quot;setAppTracingAllowed&quot;, boolean.class);        setAppTracingAllowed.invoke(null, true);        Logger.i(TAG, &quot;startTrace end&quot;);    &#125; catch (Throwable t) &#123;        Logger.e(TAG, t);    &#125;&#125;\n\n注意BuildConfig.TRACE需要在 App 模块的构建脚本 build.gradle 的buildTypes中配置生成，在线下 Release 包开启 Trace，在发布到市场的包中关闭 Trace，以免被黑产利用。\n相对应的，我们需要调整我们的封装类：\npublic static void begin(String sectionName) &#123;    startTrace();    if (initialized) &#123;        ...;    &#125;&#125;public static void end() &#123;    if (initialized) &#123;        ...;    &#125;&#125;public static void beginAsync(String methodName, int cookie) &#123;    startTrace();    if (initialized) &#123;        ...;    &#125;&#125;public static void endAsync(String methodName, int cookie) &#123;    if (initialized) &#123;        ...;    &#125;&#125;public static Async async(String biz) &#123;    startTrace();    ...;&#125;\n\n4、Trace 分析4.1、使用图形界面分析正式操作之前先掌握一些基本操作技巧。\n基本操作 1，打开 Trace 文件后，可以整体上观察哪些线程有问题，比如耗时久、Uninterruptiable Sleep (non-IO)等，定位到有问题的线程或者 Trace 段后，我们就可以对线程和 Trace 段进行更详细的分析了，如图十五所示：\n（图十五）\n\n基本操作 2，在对线程和 Trace 段进行详细分析之前，对要分析的进程和线程做下钉操作（如图十六所示）：\n\n在列表中点击要分析的 App 进程，点击后该进程会展开子列表，展示所有线程。\n找到RenderThread（渲染线程）并点击名称右侧的钉图标，把该线程钉在列表顶部。\n\n（图十六）\n\n这样做的目的是方便观察主线程和渲染线程发生了什么，找到性能瓶颈所在。\n下面我们一起看 3 个例子。\n4.1.1、Uninterruptible Sleep (non-IO)我们看一个最经典的方法耗时分析，看过 Framework 源码的同学都知道系统层Choreographer#doFrame()是执行一帧绘制（包括layout、measure、draw），如果我们页面操作时（比如浏览信息流）略显卡顿，那么必定存在某一帧或者更多帧耗时久的情况。用 Perfetto UI 打开 Trace，一眼就能看到有些些 Trace 段比较长，选中该段 Trace（这个 Trace 是系统层加的），如图十七所示：\n（图十七）\n\n观察 Slice Details 后会很一目了然的看到该 Trace 的耗时：\n\n看到这一帧耗时 177ms，很明显这么久的一帧会肉眼可见的卡顿一下。\n查看 Duration 列表，可以看到有意外，存在耗时较久的Sleeping和Uninterruptible Sleep (non-IO)。\nRunning 比较久，可以点击箭头展开查看细节。\n\n很明显，主线程存在不符合预期的Uninterruptible Sleep (non-IO)状态，这个 case 中比较明显的棕色段就是Uninterruptible Sleep的信息（不明显的放大横轴段就能看到）。\n重点来了，如何定位是什么原因引起的 Uninterruptible Sleep？我们可以这样操作，以Uninterruptible Sleep为中心放大横轴，到足够大了可以看到 App 主线程行慢慢露出Runnable和Running，如图十八所示：\n（图十八）\n\n\nRunnable不表示线程，而是表示当前线程可运行了。\nRunning表示当前线程正在运行。\n\n此时我们选中距离Uninterruptible Sleep最近的Runnable，如图十九所示：\n（图十九）\n\n在Selection Details中可以看到Waker信息，它表示唤起当前线程（这里是主线程）的线程，可以看到这里是dp2ndk线程阻塞了主线程。同时，我们在 App 进程子列表中找到dp2ndk线程，并钉住。此时dp2ndk、渲染线程、主线程等就会同时钉在顶部，方便我们做对比确认信息。\n从图十五可以看到，dp2ndk线程占用了 CPU，这里需要做优化。\n4.1.2、monitor contention with owner在 Perfetto UI 打开 Trace，放大缩小横轴到适当大小，横向滑动 Trace 段，可以看到类似monitor contention with owner [xxx]的 Trace 段，它会阻塞主线程或者渲染线程，点击主线程或者渲染线程后面的Runnable观察Waker，可以看到是哪个线程阻塞的，如图二十所示：\n（图二十）\n\n此时我们选中距离Uninterruptible Sleep最近的Runnable，从图十六的Slice Details中可以看到，NetworkKit-GRS_线程占用了 CPU，这里需要做优化。\n4.1.3、Lock contention on InternTable lockLock contention on [xxx]的类型比较多，都属于锁操作，在 App 启动过程中或者页面操作过程中非常常见，如图十七所示，发现某个 Trace 耗时较长时，单独选中该段 Trace，在 Slices 中会列出子 Trace 段列表，可以更好的看出耗时 Trace。\n类似Lock contention on InternTable lock的渲染帧会影响页面流畅性，造成 App 卡顿，如图二十一所示：\n（图二十一）\n\n4.2、使用 SQL 聚合分析官网文档：\n\n常见查询：https://perfetto.dev/docs/analysis/common-queries\n语法：https://perfetto.dev/docs/analysis/perfetto-sql-syntax\n表字段介绍：https://perfetto.dev/docs/analysis/sql-tables。\n\n使用图形界面只能一个个 Case 具体分析，当我们梳理出具体的多个 Cas 时，有时候需要拿到聚合数据给对应同事做优化，可以使用 SQL 查询。\n例如，统计名为renderConvertView[smart_ui_jiangliu]的 Trace 的平均耗时：\nSELECT name, AVG(dur)/1000 AS dur FROM slice where name = &#x27;renderConvertView[smart_ui_jiangliu]&#x27; GROUP BY name\n\n结果如图二十二所示：\n（图二十二）\n\n例如，统计名为Lock contention on thread list lock (owner tid: 4054)的 Trace，并按照耗时降序排序：\nSELECT ts, dur, name FROM slice where name = &#x27;read from memory&#x27; ORDER BY dur DESC\n\n结果如图二十三所示：\n（图二十三）\n\n5、结语本文介绍了 Perfetto 配置生成、抓取 Trace、使用 Perfetto UI 分析 Trace 三块内容。在 Trace 分析时主要介绍了基于 CPU 的分析，没有介绍 GPU、内存、电量分析的细节，因为日常工作中本人用的不是很多，所以暂时写不了就不误导读者了，有机会再补充一篇文章。\n本文介绍的内容是不到官网的 1&#x2F;3 的，但是一些细节也是官网没有的，很多人在实操时就卡在这些细节上，这也是我写这篇文章的原因。\n感兴趣的读者可以加我的 QQ 交流群：46523908。\n本文完，感谢阅读。\n","categories":["Android","性能优化"],"tags":["Prefetto"]},{"title":"Android性能优化之绑定RenderThread到大核CPU","url":"/post/20241221/1f3fc18c6801/","content":"其实本文题目更合适叫做《Android 绑定任意线程到任意 CPU》，其范围也是涵盖了题目的，但是不能体现其最大价值性能优化，因此还是缩小了涵盖范围，但是读完本文你还是可以做到 Android 绑定任意线程到任意 CPU 的。\n\n\n\n\n绑定线程到 CPU 的核心就是调用sched_setaffinity函数，但是我们要为此做一些准备：\n\n了解sched_setaffinity函数，确定绑定某线程到某 CPU 上需要什么参数。\n了解手机多核 CPU 架构，如何获取到每个 CPU 的频率，频率越高的表示性能越高，相反则越差。\n了解 Android 系统中某个进程下的所有线程的各种信息，包括线程 main、RenderThread 等系统线程。\n确定目标线程运行在哪个 CPU 上，用于确定结果。\n\n1、sched_setaffinity 函数该函数用于设置指定进程的 CPU 亲和性，设置线程和 CPU 亲和性应该使用pthread_setaffinity_np，然而在实践中我发现 Android 中没有pthread_setaffinity_np，Android 开发中可以使用sched_setaffinity代替，为了避免偏移主题这里只详细介绍sched_setaffinity。\n函数原型：\nint sched_setaffinity(pid_t pid, size_t cpusetsize, const cpu_set_t *mask);\n\n\n参数：\npid：要设置亲和性的进程的 PID。我们可以用线程 tid （内核级线程 ID）来设置线程和 CPU 的亲和性。\ncpusetsize：cpu_set_t 类型的大小。\nmask：指向一个 cpu_set_t 的指针，表示进程可以运行在哪些 CPU 上。\n\n\n\n2、CPU 频率2.1、认识 CPU 频率目前市面上几乎所有手机都是多核，我们用 Perfetto 随便抓个 Trace 就可以看到 CPU 被占用的情况，如图一所示：\n\n不了解 Perfetto 如何分析 Trace 的可以看我文章：https://www.yanzhenjie.com/post/20240406/b3b882a8973b/\n\n（图一）\n\n可以看到我这个手机有 8 个 CPU，其中 CPU7 看起来是最繁忙的，在这个手机上，它是唯一的大核 CPU，也是 main 线程所绑定的 CPU。\n对于任何 Android 手机，我们可以在/sys/devices/system/cpu/目录下看到所有 CPU：\nhw_arm64:/ $ cd /sys/devices/system/cpu/hw_arm64:/sys/devices/system/cpu $ lscpu0  cpu1  cpu2  cpu3  cpu4  cpu5  cpu6  cpu7  offline  online  cpufreq  cpuidle  ...hw_arm64:/sys/devices/system/cpu $\n\n也可以查看每个 CPU 的频率，以一个 8 核 8 线程的 CPU 为例：\nhw_arm64:/sys/devices/system/cpu $ cat cpu0/cpufreq/cpuinfo_max_freq 1804800hw_arm64:/sys/devices/system/cpu $ cat cpu1/cpufreq/cpuinfo_max_freq 1804800hw_arm64:/sys/devices/system/cpu $ cat cpu2/cpufreq/cpuinfo_max_freq 1804800hw_arm64:/sys/devices/system/cpu $ cat cpu3/cpufreq/cpuinfo_max_freq 1804800hw_arm64:/sys/devices/system/cpu $ cat cpu4/cpufreq/cpuinfo_max_freq 2419200hw_arm64:/sys/devices/system/cpu $ cat cpu5/cpufreq/cpuinfo_max_freq 2419200hw_arm64:/sys/devices/system/cpu $ cat cpu6/cpufreq/cpuinfo_max_freq 2419200hw_arm64:/sys/devices/system/cpu $ cat cpu7/cpufreq/cpuinfo_max_freq 2841600\n\n可以看出来，根据 CPU 频率的不同，可以按照将 CPU 频率分为 3 组，2841600 是唯一一个大核 CPU，也可以看出上文提到的结论。\n至此，我们可以得出 1 个简单的结论：我们可以在代码中获取到手机的 CPU 列表，并可以按照 CPU 频率对 CPU 进行分组。\n2.2、按频率分组 CPU 列表根据以上规则，我们可以按照 CPU 频率给 CPU 列表分组：\n./├── 2841600│   └── cpu7├── 2419200│   ├── cpu6│   ├── cpu5│   └── cpu4└── 1804800    ├── cpu3    ├── cpu2    ├── cpu1    └── cpu0\n\n在 C++ 中按照规则读取到这个按频率分组的 CPU 列表：\n#include &lt;fstream&gt;#include &lt;string&gt;#include &lt;map&gt;#include &lt;vector&gt;#include &lt;dirent.h&gt;#include &lt;regex&gt;#include &quot;CPUUtils.h&quot;#include &quot;../log/Log.h&quot;#define TAG &quot;CPUUtils&quot;const std::regex CPUUtils::cpuNamePattern(&quot;^cpu[0-9]+$&quot;);/**  * 获取CPU频率分组map。  *  * @return 返回非null的CPU频率分组map，key为频率，value为同频CPU列表。例如：&#123;&quot;888&quot;:[&quot;cpu0&quot;,&quot;cpu1&quot;], &quot;666&quot;:[&quot;cpu2&quot;,&quot;cpu3&quot;]&#125;。  */std::map&lt;int, std::vector&lt;std::string&gt;, std::greater&lt;&gt;&gt; CPUUtils::findCPUFreqMap() &#123;    const std::string cpuDirPath = &quot;/sys/devices/system/cpu&quot;;    std::map&lt;int, std::vector&lt;std::string&gt;, std::greater&lt;&gt;&gt; freqCPUMap;    const char *dirPath = cpuDirPath.c_str();    DIR *dir = opendir(cpuDirPath.c_str());    if (!dir) &#123;        LOGE(TAG, &quot;findCpuFreqMap, the system/cpu [%s] can be not found.&quot;, dirPath);        return freqCPUMap;    &#125;    // 遍历CPU目录下的CPU列表    struct dirent *entry;    while ((entry = readdir(dir)) != nullptr) &#123;        const std::string cpuFileName = entry-&gt;d_name;        // 只看 CPU+数字的目录，这才是CPU信息目录        if (!std::regex_match(cpuFileName, CPUUtils::cpuNamePattern)) &#123;            continue;        &#125;        // 获取CPU最大频率文件        std::string maxFreqFilePath = cpuDirPath;        maxFreqFilePath.append(&quot;/&quot;);        maxFreqFilePath.append(cpuFileName);        maxFreqFilePath.append(&quot;/cpufreq/cpuinfo_max_freq&quot;);        std::ifstream maxFreqFile(maxFreqFilePath);        if (!maxFreqFile) &#123;            LOGE(TAG, &quot;findCpuFreqMap, the cpuinfo_max_freq [%s] can be not found.&quot;, maxFreqFilePath.c_str());            continue;        &#125;        // 读取CPU最大频率，并按照频率分组记录        std::string line;        if (std::getline(maxFreqFile, line)) &#123;            try &#123;                int frequency = std::stoi(line);                freqCPUMap[frequency].push_back(cpuFileName);            &#125; catch (const std::exception &amp;e) &#123;                LOGE(TAG, &quot;findCpuFreqMap, error occurred on parse frequency: %s.&quot;, e.what());            &#125;        &#125;    &#125;    closedir(dir);    return freqCPUMap;&#125;\n\n接着上一个方法，可以获取到指定级别的 CPU 列表：\n/**  * 获取大中小核CPU列表。  *  * @param level CPU等级，0为大核，1为中核，2为小核。  * @param upward CPU序号是否向上兼容，例如如果2找不到则向上找1或者0。  *  * @return 返回非null的CPU列表，例如：[&quot;cpu0&quot;, &quot;cpu1&quot;]。  */std::vector&lt;std::string&gt; CPUUtils::findCPUListByLevel(const int level, const bool upward) &#123;    // 获取CPU频率分组    const auto findCPUFreqMap = CPUUtils::findCPUFreqMap();    if (findCPUFreqMap.empty()) &#123;        return &#123;&#125;;    &#125;    // 把std::map的key（频率）按照升序存到std::vector    std::vector&lt;int&gt; freqList;    freqList.reserve(findCPUFreqMap.size());    for (const auto &amp;pair: findCPUFreqMap) &#123;        freqList.push_back(pair.first);    &#125;    // 如果指定级别不存在，考虑是否向上兼容    const int size = static_cast&lt;int&gt;(freqList.size());    const int freqIndex = size &gt; level ? level : (upward ? (size - 1) : -1);    // 若指定级别存在，则返回频率对应的CPU列表    if (freqIndex &gt; -1) &#123;        const int freq = freqList[freqIndex];        auto it = findCPUFreqMap.find(freq);        if (it != findCPUFreqMap.end()) &#123;            return it-&gt;second;        &#125;    &#125;    return &#123;&#125;;&#125;\n\n3、RenderThread3.1、认识 RenderThreadRenderThread 是 Android 系统中一个专门用于处理 View 渲染工作的线程，它的引入主要是为了优化渲染效率和提升用户体验。在 Android 5.0（Lollipop）之前，Android 应用程序的 main 线程同时负责处理 UI 绘制和 Open GL 渲染等任务。这可能导致 main 线程在渲染复杂 UI 时出现阻塞，影响应用程序的流畅性。从 Android 5.0 开始，Android 系统引入了 RenderThread，将 UI 渲染任务从主线程中分离出来，从而提高了渲染效率和流畅度。\n\nRenderThread 主要功能：\n\n\nRenderThread 负责处理 View 的渲染工作，包括绘制、合成和显示等操作。\n它使用双缓冲机制进行渲染，能够减少屏幕闪烁和卡顿现象。\nRenderThread 还可以提前对 View 进行预渲染，减少渲染任务的延迟。\n\n\nRenderThread 工作原理：\n\n\n当应用程序需要渲染 UI 时，主线程会创建一个渲染任务并提交给 RenderThread。\nRenderThread 在独立的线程中执行渲染任务，并使用 GPU 进行硬件加速渲染。\n渲染完成后，RenderThread 将渲染结果提交给 SurfaceFlinger 进程进行合成和显示。\n\n\nRenderThread 与 main 线程的区别\n\n\n职责不同：main 线程是 Android 应用程序的主入口点，负责处理用户输入、事件分发和 UI 更新等任务。它是应用程序的核心线程，负责执行应用程序的主要逻辑。RenderThread 则专注于处理 View 的渲染工作，减轻主线程的负担，提高渲染效率和流畅度。\n并行处理：main 线程通常是顺序执行任务的，而 RenderThread 可以并行处理多个 View 的渲染任务。这意味着即使某个 View 的渲染任务比较复杂或耗时较长，也不会阻塞其他 View 的渲染或主线程的执行。\n硬件加速：RenderThread 利用 GPU 进行硬件加速渲染，能够显著提高渲染速度和效率。而 main 线程则不直接参与硬件加速渲染过程。\n交互方式：main 线程与 RenderThread 之间通过特定的接口和机制进行交互。例如，主线程可以通过提交渲染任务给 RenderThread 来触发渲染操作，而 RenderThread 则可以通过回调等方式将渲染结果通知给主线程。\n\n3.2、RenderThread 默认未绑定 CPU回到主题，我们来观察 RenderThread 运行在哪个 CPU 上，找到应用的 RenderThread，并在 RenderThread status 横轴找到相对比较繁忙的阶段，选中 Running 阶段，如图三所示它运行在CPU 0，也就是小核 CPU：\n（图二）\n\n偶尔又运行在中核 CPU 或大核 CPU，如图三所示，运行在大核 CPU：\n（图三）\n\n这就是本文题目的原因，如果 RenderThread 一直运行大核 CPU 组，那么渲染性能会不好好一点？\n4、获取任意线程 tid对于本文的场景来说，线程有两类，“我”的线程是可以拿到线程实例的，“他”的线程无法拿到线程实例。\n可以获取到实例的：\n\n当前线程，可以拿到实例，无论 C++ 还是 Java。\n由“我”创建的线程，可以拿到实例，无论 C++ 还是 Java。\n\n无法获取到实例的：\n\n比如 main 线程、RenderThread 线程。\n\n4.1、获取当前线程 tid在 C++ 中，获取当前线程的 tid（包括 C++ 的线程或者 Java 的线程），或者由“我”创建&#x2F;启动的线程是可以直接拿到 tid 的。\n获取当前线程的 tid：\n#include &lt;unistd.h&gt;int ThreadUtils::bindThreadToCPU(const int cpuNumber) &#123;    pid_t tid = gettid();    ...;&#125;\n\n获取“我”创建的线程的 tid，使用pthread_gettid_np函数：\nvoid* thread_function(void* arg) &#123;    ...;    return nullptr;&#125;void startWork() &#123;    pthread_t thread;    int result = pthread_create(&amp;thread, nullptr, thread_function, nullptr);    if (result != 0) &#123;        LOGI(TAG, &quot;pthread_create failed: %d&quot;, result);        return;    &#125;    pid_t tid = pthread_gettid_np(thread);    LOGI(TAG, &quot;Created thread tid: %d&quot;, tid);    pthread_join(thread, nullptr);&#125;\n\n4.2、获取其他线程的 tid当应用启动后，该应用的所有线程信息都位于/proc/$&#123;process_id&#125;/task下，结构如下：\n├──├── thread_id1│   └── status├── thread_id2│   └── status├── thread_id3│   └── status├── thread_id4│   └── status└── thread_id5    └── status\n\n我们可以先在电脑上做一下验证。\n首先，我们查看目标应用的进程 id：\nHarry: ~ adb shell ps | grep com.yanzhenjie.androidu0_a130        3303    300 14463340 160012 0                  0 S com.yanzhenjie.android\n\n然后进入该目录，查看线程列表：\nhw_arm64:/ cd /proc/3303/taskhw_arm64:/proc/3303/task ls3303  3313  3314  3315  3316  3317  3318  3319  3320  3321  3322  3323  3324  3325  3333  3374  3375  3378  3465\n\n打印某个线程的信息（为方便理解去掉了真实信息，仅保留结构和描述）：\nhw_arm64:/proc/3303/task cat ./3303/statusName:       &lt;线程名&gt;State:      &lt;状态&gt; (&lt;状态解释&gt;)Tgid:       &lt;线程组ID&gt;Ngid:       &lt;线程组ID&gt;Pid:        &lt;线程ID&gt;PPid:       &lt;父线程ID&gt;TracerPid:  &lt;跟踪进程ID&gt;Uid:        &lt;用户ID&gt; &lt;有效用户ID&gt; &lt;真实用户ID&gt; &lt;已保存用户ID&gt;Gid:        &lt;组ID&gt; &lt;有效组ID&gt; &lt;真实组ID&gt; &lt;已保存组ID&gt;FDSize:     &lt;文件描述符表大小&gt;Groups:     &lt;所属组ID列表&gt;NStgid:     &lt;命名空间线程组ID&gt;NSpid:      &lt;命名空间线程ID&gt;NSpgid:     &lt;命名空间进程组ID&gt;NSsid:      &lt;命名空间会话ID&gt;VmPeak:     &lt;虚拟内存峰值&gt;...\n\n我们主要关注：\nName:   RenderThreadPid:    3314\n\n到此我们就可以获取到指定名称线程的 tid 了，这里的 pid 就是内核 tid，和/proc/3303/task目录下的子目录名称一致。\n我们可以以线程名称去对应文件和 Name 做匹配，如果匹配到代表上级目录名称或者 Pid 就是内核 tid：\n/**  * 根据线程名称获取线程id。  *  * @param threadName 目标线程名称。  *  * @return 没找到返回-1，找到返回线程id。  */int ThreadUtils::findThreadId(const std::string &amp;threadName) &#123;    const int pid = getpid();    const std::string taskDirPath = &quot;/proc/&quot; + std::to_string(pid) + &quot;/task&quot;;    LOGD(TAG, &quot;findThreadId on path: %s&quot;, taskDirPath.c_str());    DIR *dir = opendir(taskDirPath.c_str());    if (dir == nullptr) &#123;        LOGE(TAG, &quot;findThreadId, dir [%s] can be not found.&quot;, taskDirPath.c_str());        return -1;    &#125;    // 遍历线程目录列表    struct dirent *entry;    while ((entry = readdir(dir)) != nullptr) &#123;        std::string threadId = entry-&gt;d_name;        if (threadId == &quot;.&quot; || threadId == &quot;..&quot;) &#123;            continue;        &#125;        std::string statusFilePath = taskDirPath;        statusFilePath.append(&quot;/&quot;);        statusFilePath.append(threadId);        statusFilePath.append(&quot;/status&quot;);        std::ifstream statusFile(statusFilePath);        if (statusFile) &#123;            std::string line;            // 遍历status文件中的每一行，找到Name行，并对比名称是否和指定相同            while (std::getline(statusFile, line)) &#123;                if (line.find(&quot;Name&quot;) != std::string::npos) &#123;                    size_t found = line.find(&#x27;:&#x27;);                    if (found != std::string::npos) &#123;                        std::string name = Utils::trim(line.substr(found + 1));                        if (threadName == name) &#123;                            closedir(dir);                            LOGI(TAG,                                 &quot;findThreadId, id of [%s] has been found: %s.&quot;,                                 threadName.c_str(),                                 threadId.c_str()                            );                            // 找到目标线程，返回目标线程tid                            return std::stoi(threadId);                        &#125;                    &#125;                    break;                &#125;            &#125;        &#125;    &#125;    closedir(dir);    LOGE(TAG, &quot;findThreadId, id of [%s] can be not found.&quot;, threadName.c_str());    return -1;&#125;// Utils.cppstd::string Utils::trim(const std::string &amp;str) &#123;    auto start = str.find_first_not_of(&quot; \\t&quot;);    auto end = str.find_last_not_of(&quot; \\t&quot;);    if (start == std::string::npos) &#123;        return &quot;&quot;;    &#125;    return str.substr(start, end - start + 1);&#125;\n\n5、设定线程 CPU 亲和性再回顾下sched_setaffinity函数：\nint sched_setaffinity(pid_t pid, size_t cpusetsize, const cpu_set_t *mask);\n\n\n参数：\npid：要设置亲和性的进程的 PID。我们可以用线程 tid （内核级线程 ID）来设置线程和 CPU 的亲和性。\ncpusetsize：cpu_set_t 类型的大小。\nmask：指向一个 cpu_set_t 的指针，表示进程可以运行在哪些 CPU 上。\n\n\n\n所以对sched_setaffinity的封装提供 2 个参数，1 个是线程，另一个是 CPU 序号：\n/**  * 绑定当前线程到指定级别的CPU上。  *  * @param cpuNumber CPU序号。  *  * @return 绑定成功返回0，否则返回-1。  */int ThreadUtils::bindThreadToCPU(const int cpuNumber) &#123;    pid_t tid = gettid();    int bind = ThreadUtils::bindThreadToCPU(tid, cpuNumber);    return bind;&#125;/**  * 绑定指定线程到指定级别的CPU上。  *  * @param threadId 线程id。  * @param cpuNumber CPU序号。  *  * @return 绑定成功返回0，否则返回-1。  */int ThreadUtils::bindThreadToCPU(const int threadId, const int cpuNumber) &#123;    cpu_set_t mask;    CPU_ZERO(&amp;mask);    CPU_SET(cpuNumber, &amp;mask);    long max_cpus = sysconf(_SC_NPROCESSORS_CONF);    if (cpuNumber &lt; 0 || cpuNumber &gt;= max_cpus) &#123;        LOGE(TAG, &quot;bindThreadToCPU, cpu %d is invalid.&quot;, cpuNumber);        return false;    &#125;    int bind = sched_setaffinity(threadId, sizeof(mask), &amp;mask);    if (bind == 0) &#123;        LOGI(TAG, &quot;bindThreadToCPU, %d, success.&quot;, threadId);    &#125; else &#123;        LOGE(TAG, &quot;bindThreadToCPU, %d, error: %d, %s&quot;, bind, errno, strerror(errno));    &#125;    return bind;&#125;\n\n到这里，所有关键的难点和代码都给出了，剩下的自己随便封装即可，为了完整性，我还是给出所有的代码。\n6、JNI 向 Java 提供接口设定当前线程到指定级别 CPU 亲和性的接口：\n/*** 绑定当前线程到指定级别的CPU上。** @param env* @param clazz* @param cpuLevel CPU级别，0为大核，1为中核，2为小核。* @param upward CPU序号是否向上兼容，例如如果2找不到则向上找1或者0。** @return 绑定成功返回0，否则返回-1。*/JNIEXPORT jint JNICALL Java_com_yanzhenjie_android_Harry_bindCurrentThreadToCPU(    JNIEnv *env, jclass clazz, jint cpuLevel, jboolean upward) &#123;    // 获取指定级别的CPU列表    const auto cpuList = CPUUtils::findCPUListByLevel(cpuLevel, upward);    if (!cpuList.empty()) &#123;        const int cpuSize = static_cast&lt;int&gt;(cpuList.size());        const int cpuIndex = Utils::randomInt(0, cpuSize - 1);        const auto &amp;cpu = cpuList[cpuIndex];        // 去掉前面CPU，只获取num        const int cpuNum = std::stoi(cpu.substr(3));        // 设定CPU亲和性        int bound = ThreadUtils::bindThreadToCPU(cpuNum);        Log::i(env, TAG,               &quot;bindCurrentThreadToCPU, level-%s cpu %s: %s&quot;,               &#123;std::to_string(cpuLevel), std::to_string(cpuNum), std::to_string(bound)&#125;        );        return bound;    &#125;    return -1;&#125;\n\n设定指定名称的线程到指定级别 CPU 亲和性的接口，比如用于 RenderThread：\n/** * 绑定指定线程到指定级别的CPU上。 * * @param env * @param clazz * @param threadName 线程名。 * @param cpuLevel CPU级别，0为大核，1为中核，2为小核。 * @param upward CPU序号是否向上兼容，例如如果2找不到则向上找1或者0。 * * @return 绑定成功返回0，否则返回-1。 */JNIEXPORT jint JNICALL Java_com_yanzhenjie_android_Harry_bindThreadToCPU(    JNIEnv *env, jclass clazz, jstring threadName, jint cpuLevel, jboolean upward) &#123;    const char *chars = env-&gt;GetStringUTFChars(threadName, nullptr);    std::string name(chars);    env-&gt;ReleaseStringUTFChars(threadName, chars);    // 根据传入线程名称获取线程tid    const int threadId = ThreadUtils::findThreadId(name);    // 根据传入CPU级别获取CPU列表    const auto cpuList = CPUUtils::findCPUListByLevel(cpuLevel, upward);    if (threadId &gt; -1 &amp;&amp; !cpuList.empty()) &#123;        const int cpuSize = static_cast&lt;int&gt;(cpuList.size());        // 从对应级别的CPU列表随机选一个CPU        const int cpuIndex = Utils::randomInt(0, cpuSize - 1);        const auto &amp;cpu = cpuList[cpuIndex];        // 去掉前面CPU，只获取num        const int cpuNum = std::stoi(cpu.substr(3));        // 设定CPU亲和性        int bound = ThreadUtils::bindThreadToCPU(threadId, cpuNum);        Log::i(env, TAG,               &quot;bindThreadToCPU, bind [%s] to level-[%s] cpu %s : %s&quot;,               &#123;name, std::to_string(cpuLevel), std::to_string(cpuNum), std::to_string(bound)&#125;        );        return bound;    &#125;    return -1;&#125;// Utils.cppint Utils::randomInt(const int min, const int max) &#123;    std::random_device rd;    std::mt19937 gen(rd());    std::uniform_int_distribution&lt;&gt; distr(min, max);    int random_num = distr(gen);    return random_num;&#125;\n\n以下是 JNI 接口对应的 Java 接口：\n/** * 绑定当前线程到指定级别的CPU上。 * * @param cpuLevel CPU级别，0为大核，1为中核，2为小核。 * @param upward CPU序号是否向上兼容，例如如果2找不到则向上找1或者0。 * * @return 绑定成功返回0，否则返回-1。 */public static native int bindCurrentThreadToCPU(int cpuLevel, boolean upward);/** * 绑定指定线程到指定级别的CPU上。 * * @param threadName 线程名。 * @param cpuLevel CPU级别，0为大核，1为中核，2为小核。 * @param upward CPU序号是否向上兼容，例如如果2找不到则向上找1或者0。 * * @return 绑定成功返回0，否则返回-1。 */public static native int bindThreadToCPU(String threadName, int cpuLevel, boolean upward);\n\n比如绑定 RenderThread 到中核 CPU：\nint bound = Harry.bindThreadToCPU(&quot;RenderThread&quot;, 1, false);\n\n比如绑定某个工作线程到中核 CPU：\nRunnable runnable = new Runnable() &#123;    @Override    public void run() &#123;        Thread.currentThread().setName(&quot;PreloadXml&quot;);        final int bound = Harry.bindCurrentThreadToCPU(1, false);        ...;    &#125;&#125;;ThreadUtils.getWorkExecutor().execute(runnable);\n\n\n调用后记得使用 Perfetto 验证是否生效，另外要多考虑一些边界情况，比如一个 4 核 4 线程的手机，可能只有一个频率组。\n\n在实际应用中，我们的 App 可能有一个线程池会专门负责预渲染页面，比如我们可以依次绑定到中核 CPU 组的每个 CPU，提升页面渲染效率。\n本文完，感谢阅读！\n","categories":["Android","C++"],"tags":["线程优化","C++"]},{"title":"让你痛苦的是物质还是执念","url":"/post/20250204/a984f691d7ab/","content":"在快节奏的现代生活中，我门常常会感到烦恼，然而不管从宏观还是微观来看，你我的工作与生活品质可能比大多数人都要好。\n我们对某些事物的过度追求和执着，可能表现为对财富、地位、名誉的渴望，也可能体现为对完美、控制、认同的执着。这些执念不仅消耗了我们的时间和精力，更让我们在追求的过程中迷失了自我，陷入了无尽的焦虑和痛苦之中。当我们执着于某一目标时，往往会忽视过程中的美好，甚至会因为目标的未能达成而陷入深深的自责和绝望。\n相比之下，物质虽然在一定程度上能够满足我们的生理需求和部分心理需求，但它并非痛苦的根源。事实上，很多时候，我们对物质的过度追求和依赖，恰恰是因为内心的某些执念在作祟。当我们放下这些执念，学会以更加平和的心态去面对生活时，我们会发现，即使物质条件并不优越，我们依然能够感受到幸福和满足。\n也许，你我该来一场说走就走的旅行，放下了对职场成功和物质财富的执念，重新审视了自己的生活和价值观。在旅途中体会生活的真谛，学会用极少的资源去创造快乐，也学会接纳和欣赏生活中的不完美。重新找回内心的平静和力量，也在回归职场后能够以更加轻松和自信的态度去面对工作中的挑战。\n因此，学会放下那些不必要的执念，以更加开放和包容的心态去面对生活中的一切。当我们不再执着于某一目标或某一结果时，我们会更加专注于过程本身，更加珍惜和享受生活中的每一个瞬间。这样的我们，不仅能够更加轻松地走向成功，也能够在人生的旅途中收获更多的幸福和满足。\n总之，执念往往是痛苦的根源，而物质并非决定性的因素。当我们学会放下执念，以更加平和和开放的心态去面对生活时，我们会发现，成功与幸福其实就在我们的身边。\n","categories":["感想"],"tags":["感想"]},{"title":"我们为什么需要”侯亮平“","url":"/post/20250302/35cf7b02ee4a/","content":"很多人有疑惑，侯亮平一个处长，怎么会受到作为省委书记的沙瑞金去专门迎接？可能沙瑞金也不是很明白上面的用意，可既然让他这么做，那一定是有各种深意在其中。\n刘新建被逮捕之前，他说过这样一句话：”你知道吗？这个新中国是谁打下来的？是我爷爷、我姥姥他们那一辈无产阶级革命家抛头颅洒热血换来的，不是你，你是坐享其成者。”这句话撕开了血淋淋的真相：侯亮平赶上了好时代，盛世里再犯错也能平步青云；侯亮平有个好妻子钟小艾，让他的冒失莽撞总能一次又一次的化险为夷。\n不可否认，侯亮平是这个时代的既得利益者。当人们逐渐理解了祁同伟后，开始厌恶侯亮平高高在上的姿态，痛恨他道貌岸然的正义。可这个国家偏偏需要这样的人，你很难见到”幸运、努力与赤子之心”同时出现在一个人身上，正是这样的人，才能永远秉持正义可守信念。正是钟小艾的通天背景，才让这株温室玫瑰敢于在狂风暴雨中绽放。他的鲁莽需要家族荫蔽托底，他的热血需要红色血脉保温。\n穷人当不了清官，只有一生顺遂无忧，身后永远站着能兜底的靠山，才敢在高速公路上拦李达康的专车，才敢揪着老学长和老师不放手。”富贵不能淫，贫贱不能移，威武不能屈”——这种教科书里的品格，在侯亮平身上成了现实，也只有这样的人，才没有破绽，即便长嘶穿怀也不过是鼓气骨血。因为无需向现实低头，所以浑身没有破绽；因为不曾被生活磋磨，所以连伤口都透着理想主义的血色。\n可惜这样的人终究是凤毛麟角，他们活在真空般的理想国度里，连呼吸都带着不真实的纯粹。有一句诛心的话：侯亮平永远体会不到菜市场里为三毛钱争执的窘迫，理解不了农民工蜷缩在桥洞下的寒凉。不是他冷漠，而是他的金丝笼里从未飞进过真正的风雨。\n所以大家应该明白了：为什么一个副厅级干部需要省委书记亲自迎接？因为寒冬需要这样的火种，盛世需要这样的镜子。我们这些在名利场摆渡的人，早把腰弯成了问号。而侯亮平们始终站成惊叹号，用笔直的脊梁丈量着理想与堕落的距离——这或许就是时代留给清白的最后一块”特权”。\n","categories":["感想"],"tags":["感想"]},{"title":"我们为什么不能原谅日本","url":"/post/20250918/0f15945abd34/","content":"\n今时今日，非你我莫属！勿使中华民族断绝，务要使夷狄不敢小视中国，乱臣贼子不敢窥测神器！\n\n一、为什么我们就是不肯原谅日本呢？随着南京照相馆和731的相继上映，网上一小部分人就跳了出来，说事情都过去80年了，为什么我们就是不肯原谅日本呢？\n答案就两句话：我们没有资格替先辈去原谅，我们也必须时刻让后辈保持警醒！\n今天是9月18日，94年前的今天，是日本帝国主义侵华的开端，也是无数先辈们14年浴血抗战的起点，这一天对中国人来说，意义太沉重了，我们绝不允许历史重演，你我千万不要觉得现在国家强大了，日子太平了，就不当回事了。\n拉开历史的尺度来看，唐朝、元朝、明朝、清朝和民国时期，日本都和我国发生过事关生死的国战，他们一直都妄图把中华大地据为己有。鲁西细菌战，南京大屠杀万人坑遗址等等，也表明了日本人是冲着亡国灭种来的，日本狭隘的国土、贫瘠的资源、频繁的地震与海啸，就注定了日本人永远不会安分。\n就像三体中的三体人，世世代代都在想着星际移民一样，目的都是要鸠占鹊巢，我们不能以常规的视角去看待日本，多灾多难且狭隘的地理环境，已经塑造出了一种另类的民族性格：一面是隐忍，一面是暴力。你若强大，他便折服，你若虚弱，他便趁虚而入。\n二、中日国战史我们先来回顾一下自唐朝以来的中日国战史。\n（一）、唐朝时期白江口之战（公元663年）\n唐朝与新罗联合对抗日本支援百济，唐军以1.3万人170艘战舰与日军4万人1000艘战舰在白江口决战，唐军以火攻和战术优势大败日军，最终日军战败，百济灭亡，日本被迫退回本土。\n日本此后近千年未敢大规模侵略中国，转而学习唐朝文化，如遣唐使制度，唐朝也巩固了在朝鲜半岛的影响力。\n（二）、元朝时期元日战争（1274年、1281年）\n元朝时期，日本当时处于镰仓幕府统治下，幕府将军北条时宗拒绝接受“藩属”地位，镰仓幕府两次拒绝元朝的“诏书”和使者，甚至杀害元朝使节。\n元朝忽必烈于1274年发起第一次远征，史称文永之役，元军3.2万人进攻九州，因后勤不足和风暴而受挫。1281年第二次远征，史称弘安之役，元军14万人（含高丽士兵）渡海，遭遇台风“神风”摧毁舰队。最终元军两次都失败，损失惨重，而使日本民族认同感增强，元朝放弃征服计划，中日关系暂时缓和。\n（三）、明朝时期壬辰倭乱（1592-1598年）\n壬辰倭乱也就是明朝时期万历朝鲜战争。明朝末年，明神宗朱翊钧连年不上朝，国家一片混乱，民变频发，财政几近崩溃，日本人发现了可乘之机，于公元1592年，日本前后出动军队总共约30万人进攻朝鲜，万历朝鲜战争爆发。\n而日军攻打朝鲜也只是幌子，当时日军统帅丰臣秀吉猖狂表态攻打朝鲜的目的“就是为了借道朝鲜进入辽东，再直取明都北京，最终目的就是最终征服中国全境”。\n丰臣秀吉曾扬言“图朝鲜窥视中华，此乃臣之素志”。\n当时明朝应藩属国朝鲜请求出兵援朝。明朝与朝鲜联军多次击败日军，但因日军据守坚固城堡（如蔚山城）和制海权劣势，战局胶着。其中有2次关键战役，1593年平壤大捷和碧蹄馆之战。日军于1598年因丰臣秀吉病逝而撤退，明朝未彻底驱逐日军，战争也几乎耗干了明朝的国力，客观上加速了明朝的灭亡。\n争贡之役（1523年）\n日本大内氏与细川氏争夺明朝朝贡贸易权，在宁波爆发冲突。期间两派使者在明朝境内械斗，演变为中日局部冲突。结果使明朝废除宁波市舶司，停止朝贡贸易，最终中日官方贸易中断，倭寇活动加剧，为后期抗倭战争埋下伏笔。\n（四）、清朝时期甲午战争（1894-1895年）\n我们都在中学课本上学过中日甲午战争，日本明治维新后，迅速实现工业化，国力增强，但资源匮乏和市场狭小的矛盾日益突出。日本将中国视为扩张的主要目标，朝鲜半岛成为其战略跳板。1894年，朝鲜爆发东学党起义，日本以“保护侨民”为名出兵朝鲜，趁机排挤清朝势力。朝鲜本为清朝附庸国，但日本通过武力控制朝鲜政权，迫使朝鲜断绝与清朝的关系。清朝出兵镇压起义，与日军在朝鲜境内爆发冲突。\n\n丰岛海战（1894年7月25日）：日本海军偷袭清朝运兵船，正式打响战争第一枪。\n黄海海战（1894年9月17日）：北洋舰队与日本联合舰队激战，北洋水师虽损失惨重，但未全军覆没。\n平壤战役（1894年9月）：清军在朝鲜平壤溃败，日军占领朝鲜全境。\n辽东战役（1894年11月-1895年1月）：日军攻占辽东半岛，旅顺大屠杀 震惊世界。\n威海卫战役（1895年1月-2月）：日军围攻北洋水师基地威海卫，北洋舰队全军覆没。\n\n结果是清朝战败，1895年4月17日，清政府与日本签订《马关条约》，主要内容包括：\n\n割让领土：割让辽东半岛、台湾及澎湖列岛，后因三国干涉，清朝以3000万两白银赎回辽东半岛。\n巨额赔款：2亿两白银（相当于清朝3年财政收入）。\n开放通商口岸：沙市、重庆、苏州、杭州等为商埠。\n允许日本在华设厂：列强援引“利益均沾”条款，掀起在华设厂潮。\n\n\n就是这2亿两白银、辛丑条约以及赔付日本的数千万两白银，让日本有了工业革命的资本，为二战日本侵华埋下了祸根。\n\n日本把这些赔款，大部分都买了舰艇飞机坦克，少部分投向了重工业，化学工业造船和冶金等领域。用历史专家井上清的话说：“没有甲午战争的赔款，日本工业化至少要推迟20年”。正是因为通过甲午战争和八国联军侵华战争，让日本人凭空获得了天量的财富，于是日本举国上下得到了一个共识，干什么都不如抢，劫来钱快呀。\n而且日本军国主义有着深厚的民众基础，你以为甲午战争只是日本少数战争狂热分子发动的吗？大错特错，甲午战争其实是一场日本全民支持的一场军事掠夺，本来当时日本政府是没有钱打这么一场大仗的，但是抵不住日本民众热情高涨呀，没有钱是吧？日本民众就疯狂的买国债，也就是借钱给国家。还不够是吧？日本民众就砸锅卖铁，直接捐赠甚至卖血筹钱给陆军上。不仅如此，日本民众还争先恐后的踊跃参军，很多学生书都不读了，要去为国尽忠，为了海对岸无数的财富以及广袤的土地，日本全民是一片疯狂呀！\n这可是去杀人呐、去抢劫呀！在这些日本民众眼中，竟然像出国务工一样正常!!!\n（五）、明国时期抗日战争（1931-1945年）\n日本侵华战争，分为局部（1931-1937）和全面（1937-1945）两个阶段。\n民国20年，也就是1931年，中国发生百年难遇的特大洪灾，灾情遍及23个省，光江淮流域灾民就高达5127万，死亡约40万人，日本人再次发现了趁虚而入的时机，自然灾害叠加宁越对峙，围剿红军等事件，而且张学良也不在东北，日本在这个时候发动战争明显利益最大。\n第一阶段为九一八事变（1931年），1931年9月18日，日本驻中国东北地区的关东军突然袭击沈阳，九一八事变爆发。随后4个多月，日军陆续占领东三省全境，建立伪满洲国。\n第二阶段为全面抗战（1937年），日军攻占北平、上海、南京（南京大屠杀），我中国军队在台儿庄、武汉等地抵抗。抗日战争期间，国共合作，共产党领导敌后游击战（如百团大战），国民党主导正面战场。\n在1945年8月15日，日本宣布无条件投降，据不完全统计，抗战时期，从中国军民伤亡人数超过3500万，约占第二次世界大战伤亡总人数的1&#x2F;3，这其中就包括南京大屠杀的30万遇难者、包括731活体实验的3000个无辜百姓、包括鲁西细菌战遇难的42.7万人、包括被抓到慰安所的20万中国妇女、包括数百万英勇抗战的中国军人。\n从九一八事变到1945年日本投降这14年间，日本对华最高投入兵力约410万人，他们杀人如麻，无恶不作。\n这是中华民族的国难呀，14年间日本犯下的滔天罪行，如梦魇般萦绕在每一个中国人脑海中，近一个世纪都挥之不去。\n看完以上5个时期的国战，我相信再麻木的人都不可能无动于衷，我相信大家对日本这个国家，应该会有更加清晰的认知。\n三、中华民族的伟大复兴之经略日本\n“经略”出自《左传·昭公七年》：“天子经略，诸侯正封，古之制也。”杜预注：“经营天下，略有四海，故曰经略。”\n\n中华名族的伟大复兴，收复台湾自不必多说，如果连祖国统一都做不到，那么别说民族复兴了，估计祖宗的棺材板都压不住了！\n回到主题，依现在的局势看，至少在我们这代人，日本是没有能力再对中国发起全面进攻。但这还远远不够，日本就是一条毒蛇，一旦出现机会，他一定会猛地跳出来狠咬咱们一口。\n日本军备扩张已成客观事实，根据日本政府官方数据及国际机构报告，日本防卫预算近年持续增长：\n\n2023财年：6.8万亿日元（约460亿美元），创历史新高；\n2024财年：7.9万亿日元（约530亿美元），同比增长16.2%；\n2025财年：8.7万亿日元（约590亿美元），计划进一步扩大。\n\n具体动向包括：\n\n军事转型：2023年完成“加贺”号护卫舰改装为“准航母”第一阶段工程，2024年正式启用；\n战略协同：成立“统合作战司令部”，与美军制定联合行动预案；\n技术突破：加速研发高超音速导弹，目标明确指向区域安全格局调整。\n\n\n注：日本防卫省多次强调其军备发展为“防御性”、“应对地区威胁”，但其行动已显著偏离和平发展轨道。\n\n不是我敏感，可以说日本军国主义的火种从未熄灭，不管我们现在多么强大，都要小心提防着这个挪不走的邻居。正如《孙子兵法》所言：“兵者，国之大事，死生之地，存亡之道，不可不察也。”\n蝉都可以在地下蛰伏17年呢，何况人呢？何况是日本呢？\n因此，我们不能重犯祖宗们犯过的历史错误，在中华强盛的时候，一定要找机会彻底打断日本的脊梁骨，上可告慰列祖列宗，下可荫泽后世子孙。如果一定要有人承担历史骂名，那就由你我来承担吧！\n","categories":["感想"],"tags":["感想"]},{"title":"人工智能|各名称与概念认识","url":"/post/20240304/dc8ba6d4ee26/","content":"\n适莽苍者，三餐而反，腹犹果然；适百里者，宿舂粮；适千里者，三月聚粮。——庄周《逍遥游》\n\n上文引用了战国中期先秦道家学派的代表人物庄子的逍遥游选段，翻译为白话文大致含义是：到近郊去的人，只带当天吃的三餐粮食，回来肚子还是饱饱的；到百里外的人，要用一整夜时间舂米准备干粮；到千里外的人，要用三个月来聚集粮草。\n我相信，看到这篇文章的人，一定是致千里者。\n我最早接触人工智能相关概念的时候大概是2019年，那时Google更新了TensorFlow，当时判断人工智能在未来肯定会越来越火，就建了个人工智能交流群，没几天500人的群就满员了。\n于是，我们开始入门了。刚入门的时候遇到了很多与人工智能相关的概念，都不太了解什么意思，学习时脑袋就比较混乱，幸好我在CS领域有多年深耕经验，知道入门一个新领域时最好是对其有宏观的认识，然后再有计划的学习细分领域。这也是我发这篇文章的原因，希望这篇文章可以帮助到后来的入门者。\n1、人工智能相关概念关于人工智能的发展历史就不再介绍了，从人工智能的定义和历史发展来看，它是一门内容广泛的学科。因此，在日常交流中，大家常常存在将人工智能、机器学习、深度学习、监督学习、强化学习、主动学习等混用的情况。\n实际上，机器学习是人工智能的一个分支，它能够从数据中自动分析获得规律，对未知数据进行预测。而人工神经网络则是人类模仿生物神经网络的结构和功能提供的一种计算模型，属于机器学习算法的一种。2012年，AlexNet获得ImageNet挑战赛冠军后，深度学习就成为了热门的机器学习算法。\n1.1、人工智能的几个重要概念1.1.1、监督学习监督学习的任务是学习一个模型，这个模型可以处理任意的一个输入，并且针对每个输入都可以映射输出一个预测结果，这里模型就相当于数学中一个函数，输入就相当于数学中的X，而预测的结果就相当于数学中的Y，对于每一个X，都可以通过一个映射函数映射出一个结果。\n1.1.2、非监督学习非监督学习是指直接对没有标记的训练数据进行建模学习。注意，在这里的数据是没有标记的数据，与监督学习最基本的区别之一就是建模的数据是否有标签。例如聚类（将物理或抽象对象的集合分成由类似的对象组成的多个类的过程）就是一种典型的非监督学习，分类就是一种典型的监督学习。\n1.1.3、半监督学习当有标记的数据很少、未被标记的数据很多，人工标记又比较昂贵时，可以根据一些条件（查询算法）查询一些数据，让专家进行标记，这是半监督学习与其他算法的本质区别。所以说对主动学习的研究主要是设计一种框架模型，运用新的查询算法查询需要专家来人工标注的数据，最后用查询到的样本训练分类模型来提高模型的精确度。\n1.1.4、主动学习当使用一些传统的监督学习方法做分类处理时，通常是训练样本的规模越大，分类的效果就越好。但是在现实中的很多场景中，标记样本的获取是比较困难的，因为这需要领域内的专家来进行人工标注，所花费的时间成本和经济成本都很大。而且，如果训练样本的规模过于庞大，则训练花费的时间也会比较多。那么问题来了：有没有一种有效办法，能够使用较少的训练样本来获得性能较好的分类器呢?答案是肯定的，主动学习（ActiveLearning）提供了这种可能，主动学习通过一定的算法查询出最有用的未标记样本，并交由专家进行标记，然后用查询到的样本训练分类模型来提高模型的精确度。\n在人类的学习过程中，通常利用已有的经验来学习新的知识，又依靠获得的知识来总结和积累经验，经验与知识不断交互。同样，机器学习就是模拟人类学习的过程，利用已有的知识训练出模型去获取新的知识。并通过不断积累的信息去修正模型，以得到更加准确有用的新模型。不同于被动地接受知识，主动学习能够有选择性地获取知识。\n1.2、机器学习的分类根据不同的划分角度，可以将机器学习划分为不同的类型。\n1.2.1、按任务类型划分机器学习模型按任务类型可以分为回归模型、分类模型和结构化学习模型。具体说明如下：  \n\n回归模型：又叫预测模型，输出的是一个不能枚举的数值。  \n分类模型：又分为二分类模型和多分类模型，常见的二分类问题有垃圾邮件过滤，常见的多分类问题有文档自动归类。  \n结构化学习模型：此类型的输出不再是一个固定长度的值，如图片语义分析输出是图片的文字描述。\n\n1.2.2、按方法划分机器学习按方法可以分为线性模型和非线性模型，具体说明如下：  \n\n线性模型：虽然比较简单，但是其作用不可忽视，线性模型是非线性模型的基础，很多非线性模型都是在线性模型的基础上变换而来的。  \n非线性模型：又可以分为传统机器学习模型（如SVM、KNN、决策树等）和深度学习模型。\n\n1.2.3、按学习理论划分机器学习模型可以分为有监督学习、半监督学习、无监督学习、迁移学习和强化学习，具体说明如下：  \n\n训练样本带有标签时是有监督学习。  \n训练样本部分有标签、部分无标签时是半监督学习。  \n训练样本全部无标签时是无监督学习。  \n迁移学习就是把已经训练好的模型参数迁移到新的模型上，以帮助新模型训练。  \n强化学习是一个学习最优策略（Policy，可以让本体（Agent）在特定环境（Environment）中，根据当前状态（State）做出行动（Action），从而获得最大回报（Reward），强化学习和有监督学习最大的不同是：每次的决定没有对与错，只是希望获得最多的累积奖励。\n\n1.3、机器学习和深度学习机器学习是一种实现人工智能的方法，而深度学习是一种实现机器学习的技术。深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络等），因此越来越多的人将其单独看作一种学习的方法。\n假设需要识别某个照片是狗还是猫，如果是用传统机器学习的方法，首先会定义一些特征，如有没有胡须、耳朵、鼻子和嘴巴的特征等。总之，首先要确定相应的“面部特征”作为机器学习的特征，以此将对象进行分类识别。而深度学习的方法则更进一步，它会自动找出这个分类问题所需要的重要特征，而传统机器学习则需要人工地给出特征。\n我的理解是，其实深度学习并不是一个独立的算法，在训练神经网络的时候也通常会用到监督学习和无监督学习。但是由于一些独特的学习方法被提出，也可以把它看成是单独的一种学习算法。深度学习可以大致理解成包含多个隐含层的神经网络结构，深度学习的深指的就是隐藏层的深度。\n2、神经网络2.1、CNNCNN是卷积神经网络（Convolutional Neural Network）的缩写，是一种专门为处理具有网格结构的数据（如图像）而设计的深度学习架构。CNN在图像处理和计算机视觉领域特别成功，广泛应用于图像识别、物体检测、图像分类、图像分割、视频分析和自然语言处理等任务。\nCNN的核心思想是利用卷积运算来自动和有效地提取图像中的局部特征，无需手动特征工程。卷积层通过滤波器（也叫卷积核或权重）在输入图像上滑动，计算局部区域的点积，并生成特征图（feature map）来表示特定的视觉特征，如边缘、角点或更复杂的纹理和形状。\nCNN通常由以下几种类型的层组成：  \n\n卷积层（Convolutional Layer）：使用卷积核提取输入数据的特征。\n激活层（Activation Layer）：通常是非线性激活函数，如ReLU，它增加了模型的非线性能力，使得网络能够捕捉更复杂的特征。\n池化层（Pooling Layer）：采用下采样操作（如最大池化或平均池化）来减少特征图的空间尺寸，减少参数数量，以及增加模型的不变性特性。\n全连接层（Fully Connected Layer）：通常放在网络的最后几层，这些层可将学习到的高层特征映射到最终的输出，例如分类的类别。\n归一化层（Normalization Layer）：例如批归一化（Batch Normalization），用于调整神经网络中间层的激活值分布，以提高训练速度和稳定性。\n丢弃层（Dropout Layer）：随机丢弃网络中的部分激活值，以避免过拟合。\n\n2.2、RNNRNN是循环神经网络（Recurrent Neural Network）的缩写，是一种专门设计来处理序列数据的人工神经网络。与传统的前馈神经网络不同，RNN能够处理任意长度的序列，因为它们具有内部状态（memory）来存储前面的信息。\nRNN设计用于处理序列数据和时间序列数据，能够对序列中的元素进行建模，考虑到时间上的依赖关系。RNN的关键在于它们的循环连接，这使得网络能够将信息从前序步骤传递到当前步骤，从而保持对先前元素的“记忆”。RNN在自然语言处理（NLP）、语音识别、时间序列预测等任务中非常有效。\nRNN的主要问题是长期依赖问题，即难以捕捉长序列中的依赖关系，这通常表现为梯度消失或梯度爆炸。为了解决这些问题，研究者开发了LSTM（长短期记忆）和GRU（门控循环单元）等更高级的循环神经网络变体。\n2.3、CNN与RNN的区别\n数据类型：CNN主要用于图像数据，而RNN主要用于序列数据。\n架构：CNN使用卷积层来提取空间特征，而RNN包含循环来处理时间序列。\n应用场景：CNN通常用于图像和视频处理任务，如图像分类、对象检测和图像分割。RNN经常用于处理时间相关的任务，如语言模型、机器翻译、语音到文本转换。\n记忆能力：CNN没有内部状态，不保存先前的输入信息，而RNN通过隐藏状态保持对过去信息的记忆。\n参数共享：CNN通过滑动卷积核实现参数共享，而RNN在时间步之间共享参数。\n变体和改进：CNN可以通过添加特殊层（如批归一化层）进一步优化，而RNN的改良通常涉及改变循环单元的结构（如使用LSTM或GRU）。\n\n2.4、其他\n多层感知器（MLP）:\n也称为密集或全连接网络，是神经网络的最基本形式。\n每个神经元与前一层的所有神经元连接，没有循环或卷积结构。\n主要用于表格数据或不需要捕获空间或时间依赖性的问题。\n\n\n自编码器（Autoencoders，AE）:\n用于无监督学习，主要用于降维和特征学习。\n由编码器和解码器两部分组成，编码器将输入压缩成一个低维表示，解码器将该表示重构回原始输入。\n\n\n变分自编码器（Variational Autoencoders，VAE）:\n类似于传统自编码器，但它们产生的是输入数据的概率分布。\n通常用于生成模型，可以生成新的、与训练数据类似的实例。\n\n\n生成对抗网络（Generative Adversarial Networks, GAN）:\n包含两个部分：一个生成器和一个鉴别器。\n生成器尝试生成逼真的数据，鉴别器尝试区分真实数据和生成器生成的数据。\n通常用于图像生成、风格转换等。\n\n\n长短期记忆网络（Long Short-Term Memory，LSTM）:\n一种特殊的RNN，通过引入门控制制解决了传统RNN的梯度消失问题。\n在序列预测和自然语言处理领域得到了广泛应用。\n\n\n门控循环单元（Gated Recurrent Unit，GRU）:\n类似于LSTM，但结构上更简单，参数更少。\n用门控机制来控制信息流，同样适用于处理序列数据。\n\n\nTransformers:\n侧重于自注意力机制（self-attention），能够处理整个输入序列并捕获远程依赖性。\n在NLP领域取得了显著的成功，如BERT、GPT等模型都是基于Transformer架构。\n\n\n图神经网络（Graph Neural Networks，GNN）:\n用于处理图形数据，如社交网络、分子结构或任何其他非欧几里得数据。\n能够捕获节点之间的连接关系和网络结构信息。\n\n\n卷积循环神经网络（Convolutional Recurrent Neural Networks, CRNN）:\n结合了CNN和RNN的特点，用于处理具有空间和时间维度的数据，如视频分析或图像序列。\n\n\nCapsule Networks (CapsNets):\n试图解决CNN在图像处理中的一些限制，如空间层次关系和视角不变性问题。\n包含“胶囊”，其中包含了关于对象状态的信息，以及动态路由协议。\n\n\n\n3、模型格式转换3.1、ONNX全称Open Neural Network Exchange，是微软主导的一个开放式深度学习神经网络模型格式。该格式旨在通过一个统一的模型标准，促进不同人工智能框架之间的互操作性，从而建立一个强大的人工智能生态。是一个用于表示深度学习模型的开放标准，它被设计成允许模型在不同的深度学习框架之间进行转换和互操作。支持TensorFlow、Keras、PyTorch、Caffe、CNTK、MXNet、Apache SINGA的转换，并且在与这些框架的集成中扮演“中间人”的角色。\n官网：https://onnx.ai/开源地址：https://github.com/onnx/onnx/  \n3.2、其他除了ONNX，目前没有广泛接受的、类似范围的通用标准，但有一些其他工具和库可以用于模型转换和优化：\n\nMMDNN（Model Management on Deep Neural Network）：\n由微软开发，它是一个跨框架的深度学习模型转换工具。\n支持多种格式之间的转换，包括但不限于TensorFlow、Keras、PyTorch、Caffe和CNTK。\n\n\nTVM：\n一个开源机器学习编译器栈，用于从多种框架（包括PyTorch、TensorFlow、MXNet和Keras）编译和优化模型，以便在各种硬件上运行。\n它提供了一个中间表示（IR）来优化模型，并可以将模型编译成高效的可执行代码。\n\n\nCoreMLTools：\nApple提供的一套工具，用于将多种深度学习模型转换为苹果设备的Core ML格式。\n支持TensorFlow、Keras、Caffe和其他框架的模型转换。\n\n\nTF-TRT (TensorFlow-TensorRT Integration)：\nTensorFlow和NVIDIA TensorRT的集成，用于加速TensorFlow模型在NVIDIA GPU上的推理。\n它自动优化和转换TensorFlow图以利用TensorRT的优化。\n\n\nXLA (Accelerated Linear Algebra)：\nTensorFlow提供的一个编译器，可以优化TensorFlow模型的计算图，以提高执行效率和推理速度。\n\n\nOpenVINO Toolkit：\n由Intel推出，用于优化和部署深度学习模型。\n支持多种格式的模型，包括ONNX、TensorFlow和Caffe，并且专门优化了在Intel硬件（如CPU、GPU和VPU）上运行。\n\n\n\n尽管上述工具和库可能不如ONNX那样专注于模型格式的标准化，但它们提供了在不同框架和平台之间转换、优化和部署模型的功能。这些工具各有侧重点，选择时应考虑模型的原始框架、目标部署平台和性能要求。\n4、模型可视化工具4.1、Netron在人工智能领域，Netron是一个图形化的工具，用于可视化和探索深度学习和机器学习模型的结构。它支持多种不同的模型文件格式，包括常见的框架如TensorFlow、Keras、PyTorch、Caffe、ONNX、CoreML等。\n\n模型可视化：通过Netron，用户可以直观地查看模型的层次结构、每层的参数和形状、连接模式等信息。\n跨平台兼容性：Netron可以作为Web应用程序在浏览器中运行，也可以作为桌面应用程序在Windows、macOS和Linux上运行。\n简单易用：用户只需将模型文件拖放到Netron中，就可以查看模型的结构。没有复杂的安装或配置过程。\n模型探索：Netron允许用户交互式地探索模型的细节，例如点击不同的层来查看其属性和参数。\n模型共享：由于Netron提供了图形化的模型表示，它可以帮助团队成员或合作伙伴之间更容易地讨论和共享模型设计。\n无需运行模型：Netron用于静态分析，因此不需要运行模型或安装深度学习框架。\n\n开源地址：https://github.com/lutzroeder/netron/软件下载：https://github.com/lutzroeder/netron/releases网页版：https://netron.app/  \n4.2、其他\nTensorBoard：\nTensorFlow提供的一个可视化工具，可以展示模型的计算图，监控训练过程中的各种指标，如损失和准确率，并分析模型的参数和性能。\n支持标量、图像、音频、直方图和嵌入向量等数据的可视化。\n\n\nTensorWatch：\n微软推出的一个灵活的可视化工具，主要用于PyTorch。\n支持实时的数据流可视化，可以在Jupyter Notebook中使用，也可以创建独立的实时图表。\n\n\nVisdom：\n由Facebook研究团队开发的一个灵活的可视化工具，可以用于创建、组织和分享实时数据的可视化。\n支持多种数据类型的可视化，包括数值、图像、文本、音频和视频数据。\n\n\nWeights &amp; Biases：\n一个机器学习实验跟踪工具，可以帮助记录和可视化模型的训练过程。\n提供了一个在线平台，适用于多种深度学习框架，支持超参数优化、模型版本控制和协作。\n\n\nWhat-If Tool：\n一个交互式可视化工具，用于分析机器学习模型的行为，尤其是在解释模型预测和公平性方面。\n可以与TensorBoard配合使用，并支持多种数据格式和模型类型。\n\n\nPlotNeuralNet：\n一个LaTeX代码库，用于从代码中生成卷积神经网络的高质量架构图。\n需要LaTeX环境来编译成图形，并允许用户自定义模型的外观和布局。\n\n\nNN-SVG：\n一个在线工具，用于快速生成神经网络架构的SVG图像。\n用户可以调整层数、神经元数量和其他属性，然后下载生成的图像。\n\n\n\n5、深度学习框架本文介绍了目前比较主流的、我个人比较感兴趣一些深度学习框架。\n5.1、TensorFlow\n发展者与社区： 由Google的Google Brain团队开发，有着非常活跃的社区和广泛的工业支持。\n接口与使用： 提供了Python、C++等多种语言的API，以及高层API如tf.keras，适合各种用户需求。\n设计和特点： 初版基于静态计算图，但从TensorFlow 2.0开始，默认支持动态计算图（Eager Execution）。\n扩展性和部署： 支持多种硬件平台，包括GPU和TPU，提供了TensorFlow Lite供移动和嵌入式设备使用。\n应用范围： 适用于广泛的机器学习任务，从研究到企业级生产均有广泛应用。\n\nTensorFlow官网：https://www.tensorflow.org/TensorFlow Lite官网：https://www.tensorflow.org/lite开源地址：https://github.com/tensorflow/tensorflow  \n5.2、Keras\n发展者与社区： 最初由François Chollet单独开发，现在是TensorFlow的官方高级API。\n接口与使用： 提供了高度抽象化的API，使得构建和训练模型变得简单快捷，适合初学者和快速原型设计。\n设计和特点： 作为TensorFlow的一部分，与TensorFlow紧密集成，同时保持了独立性和模块化。\n扩展性和部署： 作为TensorFlow的一部分，享受TensorFlow的扩展性和部署能力。\n应用范围： Keras非常适合教学和快速开发，但对于一些需要高度定制化的复杂模型，可能会使用TensorFlow的低级API。\n\nKeras在2017年被集成进TensorFlow，在TensorFlow 2.0版本中，Keras被正式确立为TensorFlow的高级API，即tf.keras。\n官网：https://keras.io/开源地址：https://github.com/keras-team/keras  \n5.3、PyTorch\n发展者与社区： 由Facebook的AI研究团队开发，并且在学术界有很大影响力和强大的社区。\n接口与使用： 提供了以Python为中心的API，充分利用了Python的动态特性，使原型设计和实验更加自然。\n设计和特点： 采用动态计算图，即定义即运行（define-by-run）方式，非常适合研究和动态变化的模型。\n扩展性和部署： 可以导出为ONNX格式，与其他工具配合实现跨平台部署。有一定的移动端支持。\n应用范围： 在学术研究中非常受欢迎，也适用于工业应用，尤其是在快速原型设计和迭代中。\n\n官网：https://pytorch.org/开源地址：https://github.com/pytorch/pytorch  \n5.4、Caffe\n发展者与社区： 由加州大学伯克利分校的BVLC团队开发，社区相对于TensorFlow和PyTorch较小。\n接口与使用： 主要使用配置文件定义模型结构，同时提供了Python和C++的API，但没有TensorFlow和PyTorch那么灵活。\n设计和特点： 高性能的前向传播和速度优化，适合于图像分类和其他视觉任务。\n扩展性和部署： 主要优化了在GPU上的部署，但在移动端和嵌入式设备支持方面不如TensorFlow和PyTorch。\n应用范围： 因其高效的性能，Caffe经常用于工业产品和科研中的模型训练和部署，尤其是计算机视觉任务。\n\n官网：https://caffe.berkeleyvision.org/开源地址：https://github.com/BVLC/caffe  \n5.5、Core MLCore ML 并不是一个开源项目，它是苹果公司开发的专有框架，用于在 iOS、macOS、tvOS 和 watchOS 上运行机器学习模型。由于它是专有的，没有公开的源代码可供下载或修改。\n苹果公司的 Core ML 官方主页提供了关于 Core ML 的详细信息和文档，包括如何开始使用 Core ML、如何转换和训练模型、以及 Core ML 支持的模型格式等。\n官网：https://developer.apple.com/machine-learning/  \n5.6、总结深度学习框架总的来说，我个人比较推荐TensorFlow和PyTorch，它们都是完整的深度学习框架，支持广泛的应用，并且它们的社区和工具都在不断发展。Caffe在某些特定的领域如计算机视觉中仍然是一个高效可靠的选择，尽管它的流行度可能不如前两者。而Keras提供了一个用户友好的接口，使得深度学习更加容易上手，是学习和快速开发的首选。\n6、国内深度学习推理引擎主要介绍几个国内开源的深度学习推理引擎。\n6.1、MNNMNN（Mobile Neural Network）由阿里巴巴出品，是一个轻量级、高性能的深度学习框架，专门针对移动端和边缘设备设计。MNN的目标是帮助开发者和企业能够在移动设备上高效地部署和运行深度学习模型，特别是在资源有限的环境下。\n\n轻量化：MNN专为移动设备优化，拥有较小的运行时库，能够减少应用的资源消耗。\n跨平台支持：MNN支持包括Android、iOS以及Linux等多个平台，同时支持ARM、x86、MIPS等不同架构的处理器。\n高性能：MNN针对移动端处理器进行了深度优化，包括对ARM CPU的NEON指令集优化、对GPU的OpenCL和Metal优化，以及对NPU（神经网络处理单元）的适配，可充分利用设备的计算资源。\n易用性：MNN提供了简洁的API，便于开发者快速集成和使用。它支持ONNX、Caffe、TensorFlow等多种模型格式，方便开发者导入和转换已有模型。\n多后端支持：MNN可以根据不同的硬件条件自动选择最佳的计算后端执行计算任务，包括CPU、GPU、NPU等。\n自动调度和内存复用：MNN内部实现了智能的算子调度和内存管理，减少内存占用和计算延迟。\n自定义算子：如果遇到MNN不支持的算子，开发者还可以自定义算子以满足特殊需求。\n\n官网：https://www.mnn.zone/开源地址：https://github.com/alibaba/MNN  \n6.2、NCNNNCNN（Neural Network Inference Framework）由腾讯出品，是腾讯优图实验室开发的高性能神经网络前向推理框架，专为移动端和边缘设备设计，具有轻量级和高效率的特点。\n\n轻量级：NCNN 设计目标是保持足够的轻量级，以便于在移动设备上快速部署和执行。\n高性能：它通过优化执行速度来尽可能利用移动设备的计算资源，包括对 ARM CPU 的 NEON 指令集、Vulkan 计算着色器、OpenCL 和多线程的支持。\n不依赖第三方库：NCNN 尽量减少依赖，不需要依赖 BLAS 或 CNN 等第三方库，这使得在移动端部署时更加简洁和方便。\n支持多种模型格式：提供了工具来转换包括 Caffe、ONNX、PyTorch 和 TensorFlow 在内的多种模型到 NCNN 的格式。\n易于集成：NCNN 提供了简单而直观的 API，使得将预训练模型集成到移动应用变得非常简单。\n跨平台支持：支持在 Android、iOS、Windows 和 Linux 等多种平台上运行。\n\n开源地址：https://github.com/Tencent/ncnn  \n6.3、MindSporeMindSpore是华为推出的一个全场景深度学习框架，移动端部署需要使用MindSpore Lite。\n\n轻量级：相较于完整版的MindSpore，Lite版本更小巧，适用于计算能力和存储空间受限的移动设备。\n跨平台支持：支持Android和iOS等移动操作系统，方便开发者将AI模型部署到各种智能手机和平板电脑上。\n优化的性能：针对移动和端侧设备进行了性能优化，以提高运算速度和效率，减少功耗。\n易用的工具链：提供工具链支持，包括模型转换工具，可以将MindSpore或其他框架（如ONNX）的模型转换为MindSpore Lite模型。\n丰富的算子库：包含了丰富的算子（操作），以支持多种AI场景下的计算需求。\n\nMindSpore官网：https://www.mindspore.cn/MindSpore Lite官网：https://www.mindspore.cn/lite开源地址：https://github.com/mindspore-ai/mindspore  \n6.4、maceMACE（Mobile AI Compute Engine）是小米开源的一个深度学习推理框架，专门设计用于在移动端和嵌入式设备上优化和运行机器学习模型。MACE支持多种平台，包括Android和iOS设备，以及支持ARM架构的Linux设备。\n\n跨平台：提供对Android和iOS操作系统的支持，使得开发者能够轻松地将训练好的模型部署到多种移动设备上。\n多后端支持：支持在CPU、GPU和DSP等多种计算单元上运行模型，利用OpenCL和OpenGL等技术实现跨设备的通用性和高性能。\n模型转换工具：包含模型转换器，可以将Caffe、TensorFlow、ONNX等格式的模型转换为MACE支持的格式。\n优化的性能：针对移动设备的计算资源进行优化，提高模型运行效率，减少延迟和功耗。\n易用性：提供了易于使用的接口和详细的文档，帮助开发者快速上手并部署他们的模型。\n算子库：包含了丰富的预定义算子（operations），满足常见深度学习模型的需求。\n\n官网：https://mace.readthedocs.io/开源地址：https://github.com/XiaoMi/mace  \n6.5、PaddlePaddlePaddlePaddle是由百度推出的开源深度学习平台，支持多种深度学习模型，并且特别优化了在百度大规模应用中的一些实用功能。\n\n跨平台兼容性：支持多种操作系统，包括Android、iOS、Linux等，并支持ARM、X86、OpenCL、Metal、CUDA等多种后端。\n轻量化设计：为了适应移动端和嵌入式设备的性能和存储限制，PaddlePaddle Lite优化了内存占用和运行速度，使之成为一个轻量级推理引擎。\n优化的性能：通过内核级优化、算子融合、内存复用等技术实现了高性能的深度学习模型推理。\n易于集成：提供了简洁的API接口，便于将PaddlePaddle Lite集成到移动端应用中。\n模型转换工具：配备了模型优化工具，可以将PaddlePaddle格式的模型转换为PaddlePaddle Lite支持的格式，并对模型进行优化以提高在端侧设备上的性能。\n\n官网：https://www.paddlepaddle.org.cn/PaddlePaddle开源地址：https://github.com/PaddlePaddlePaddle-Lite开源地址：https://github.com/PaddlePaddle/Paddle-Lite\n6.6、MegEngineMegEngine是旷视公司自主研发的深度学习框架，于2020年3月25日全面开源。MegEngine 致力于提供高性能和易用性，支持广泛的AI应用，从图像和视频分析到语音识别和自然语言处理。\n\n易用性：提供了 Python API，使得构建和训练模型的过程变得简单直观。\n高性能：通过高效的张量操作和自动微分系统，以及优化后的内存使用和计算图执行，MegEngine 旨在提供高效的模型训练和推理。\n分布式训练：支持多GPU和跨节点的分布式训练，使得处理大规模数据集和复杂模型成为可能。\n模型转换和优化：MegEngine 提供了一系列用于模型优化和量化的工具，以减小模型大小并加快推理速度，适应各种计算环境。\n\n官网：https://www.megengine.org.cn/开源地址：https://github.com/MegEngine/MegEngine  \n6.7、OneFlowOneFlow 是OneFlow Inc.（一流科技有限公司）开发病开源的，OneFlow 旨在为用户提供易用、高效和可扩展的深度学习工具，特别强调在大规模分布式环境中的性能和灵活性。\n\n性能优化：OneFlow 设计了新颖的底层框架，用于提高计算和内存利用率，尤其关注大规模分布式训练的性能优化。\n易用性：提供了类似于 PyTorch 的接口，使用户可以轻松迁移现有的 PyTorch 代码，同时保持性能优势。\n动静态图兼容：支持动态图和静态图两种编程范式，用户可以根据需求灵活选择。\n分布式训练：内置了先进的分布式训练策略，支持各种分布式训练场景，包括数据并行、模型并行和流水线并行。\n自动混合精度：支持自动混合精度训练，能够在保持模型精度的同时减少内存消耗和提高训练速度。\n\n官网：https://oneflow.org开源地址：https://github.com/Oneflow-Inc/oneflow  \n本文完。\n\n特别说明：本文部分内容来自AI大模型，如有错误请评论指出，不胜感激！\n\n\n本文参考了以下文献：\n\n《TensorFlow Lite移动端深度学习》——朱元涛\n《PyTorch神经网络实战》——丛晓峰、彭程威、章军\n\n","categories":["人工智能"],"tags":["PyTorch","CNN","RNN","TensorFlow"]}]